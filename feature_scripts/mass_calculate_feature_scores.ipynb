{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/juanp.lievanok./nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "/Users/juanp.lievanok./miniconda3/envs/constructive-ranking/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from credibility import get_credibility_subfeatures\n",
    "# from credibility import get_comment_readability\n",
    "\n",
    "from defection import get_defection_score\n",
    "from coalition import get_coalition_score\n",
    "from onesidedness import get_onesidedness_score\n",
    "from resilience import get_resilience_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_meta_data(directory):\n",
    "    dictionaries = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                subreddit, date_string = re.match(directory+'/(.+)/(.+)', root).groups()\n",
    "                date = datetime.strptime(date_string, 'date_%m-%d-%Y_time_%H-%M')\n",
    "                path = os.path.join(root, file)\n",
    "                post_id = re.match('(.+).json', file).group(1)\n",
    "                new_dict = {\n",
    "                    'path': path,\n",
    "                    'subreddit': subreddit,\n",
    "                    'download_date' : date,\n",
    "                    'post_id' : post_id\n",
    "                }\n",
    "                dictionaries.append(new_dict)\n",
    "    return dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legacy now that we're separating the credibility loop from the others. \n",
    "\n",
    "# def calculate_feature_scores(comment_forest):\n",
    "#     \"\"\"\n",
    "#     Calculataes coalition, onsidedness and defection scores,\n",
    "#     and calculates the credibility subfeatures (which have to be normalized and made into a credibility score\n",
    "#     but this normalization requires the credibility subfeatures for all the data to be calculated first.)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Compute values\n",
    "#     # TODO coalition = get_coalition_score(comment_forest) \n",
    "#     onesidedness = get_onesidedness_score(comment_forest)\n",
    "#     defection = get_defection_score(comment_forest)\n",
    "\n",
    "#     simple_features_dictionary = {\n",
    "#         # TODO \"coalition\": coalition,\n",
    "#         \"onesidedness\": onesidedness,\n",
    "#         \"defection\": defection,\n",
    "#     }\n",
    "\n",
    "#     credibility_dictionary = get_credibility_subfeatures(comment_forest)\n",
    "\n",
    "\n",
    "#     return credibility_dictionary | simple_features_dictionary\n",
    "\n",
    "\n",
    "# def mass_calculate_feature_scores(directory_path, target_directory_name = '../misc_dataframes_with_test_results'):\n",
    "    \n",
    "#     calculation_start_time = datetime.now()\n",
    "\n",
    "#     dicts = gather_meta_data(directory_path)\n",
    "#     exceptions_count = 0 \n",
    "\n",
    "#     df_rows = []\n",
    "#     for json_dictionary in dicts:\n",
    "#         with open(json_dictionary['path']) as file:\n",
    "#             try:\n",
    "#                 comment_forest = json.load(file)\n",
    "#                 feature_scores_dict = calculate_feature_scores(comment_forest)\n",
    "#                 new_row_dict = {**json_dictionary, **feature_scores_dict}\n",
    "#                 df_rows.append(new_row_dict)\n",
    "#             except Exception as e:\n",
    "#                 exceptions_count += 1\n",
    "#                 print(f'Exceptions count = {exceptions_count}')\n",
    "#                 print(e)\n",
    "                \n",
    "#     timestamp_str = calculation_start_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "#     safe_dir_name = re.sub(r'[\\\\/]', '_', directory_path) # Sanitize the directory_path to remove slashes or other problematic characters\n",
    "\n",
    "#     filename = f'credibility_subfeatures_and_simple_feature_scores_for_jsons_in_{safe_dir_name}_calculated_at_{timestamp_str}.csv'\n",
    "    \n",
    "#     target_path = os.path.join(target_directory_name, filename)\n",
    "#     df = pd.DataFrame(df_rows)\n",
    "#     df.to_csv(target_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple features (coalition, defection, onesidedness, and resilience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_simple_feature_scores(comment_forest):\n",
    "    ''' \n",
    "    Return dictionary with the \"simple feature\" scores for the given comment forest.\n",
    "    Simple feature scores are coalition, defection, onesidedness, and resilience.\n",
    "    Credibility is not simple because it requires calculating subfeatures for all data before being computed. \n",
    "    '''\n",
    "    \n",
    "    # Compute values\n",
    "    coalition = get_coalition_score(comment_forest) \n",
    "    onesidedness = get_onesidedness_score(comment_forest)\n",
    "    defection = get_defection_score(comment_forest)\n",
    "    resilience = get_resilience_score(comment_forest) \n",
    "\n",
    "    simple_features_dictionary = {\n",
    "        \"coalition\": coalition,\n",
    "        \"onesidedness\": onesidedness,\n",
    "        \"defection\": defection,\n",
    "        \"resilience\" : resilience\n",
    "    }\n",
    "\n",
    "    return simple_features_dictionary\n",
    "\n",
    "def mass_calculate_simple_feature_scores(directory_path, target_directory_name = '../misc_dataframes_with_test_results'):\n",
    "    ''' \n",
    "    Simple feature scores are coalition, defection, onesidedness, and resilience.\n",
    "    Credibility is not simple because it requires calculating subfeatures for all data before being computed. \n",
    "    '''\n",
    "    calculation_start_time = datetime.now()\n",
    "\n",
    "    dicts = gather_meta_data(directory_path)\n",
    "    exceptions_count = 0 \n",
    "\n",
    "    df_rows = []\n",
    "    for json_dictionary in tqdm(dicts):\n",
    "        with open(json_dictionary['path']) as file:\n",
    "            try:\n",
    "                comment_forest = json.load(file)\n",
    "                feature_scores_dict = calculate_simple_feature_scores(comment_forest)\n",
    "                new_row_dict = {**json_dictionary, **feature_scores_dict}\n",
    "                df_rows.append(new_row_dict)\n",
    "            except Exception as e:\n",
    "                exceptions_count += 1\n",
    "                print(f'Exceptions count = {exceptions_count}')\n",
    "                print(e)\n",
    "                \n",
    "    timestamp_str = calculation_start_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    safe_dir_name = re.sub(r'[\\\\/]', '_', directory_path) # Sanitize the directory_path to remove slashes or other problematic characters\n",
    "\n",
    "    filename = f'simple_feature_scores_for_jsons_in_{safe_dir_name}_calculated_at_{timestamp_str}.csv'\n",
    "    \n",
    "    target_path = os.path.join(target_directory_name, filename)\n",
    "    df = pd.DataFrame(df_rows)\n",
    "    df.to_csv(target_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/189 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n",
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/189 [00:03<03:11,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n",
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/189 [00:07<03:09,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 9/189 [00:08<02:53,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/189 [00:10<02:34,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 13/189 [00:11<02:24,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n",
      "entered first if case, comments < 10.\n",
      "entered first if case, comments < 10.\n",
      "entered first if case, comments < 10.\n",
      "entered first if case, comments < 10.\n",
      "entered first if case, comments < 10.\n",
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 43/189 [01:06<04:47,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 50/189 [01:18<04:28,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 52/189 [01:20<03:17,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 58/189 [01:30<04:05,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 79/189 [02:16<04:02,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 82/189 [02:20<03:16,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 84/189 [02:22<02:26,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n",
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 87/189 [02:23<01:44,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 90/189 [02:27<01:46,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 92/189 [02:28<01:38,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 94/189 [02:30<01:31,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 97/189 [02:34<01:43,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered first if case, comments < 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 118/189 [03:46<04:53,  4.14s/it]"
     ]
    }
   ],
   "source": [
    "directory_path = '../scraping/representative_subreddits_for_varied_percentiles'\n",
    "mass_calculate_simple_feature_scores(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = '../misc_dataframes_with_test_results/credibility_subfeatures_and_simple_feature_scores_for_jsons_in_.._scraping_representative_subreddits_for_varied_percentiles_calculated_at_2025-05-01_21-32-52.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625263"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['total_word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "constructive-ranking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
