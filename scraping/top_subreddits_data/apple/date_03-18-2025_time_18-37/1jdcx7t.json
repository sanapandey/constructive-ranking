{
    "title": "Mac Studio With M3 Ultra Runs Massive DeepSeek R1 AI Model Locally",
    "author": "iMacmatician",
    "subreddit": "apple",
    "rank": 11,
    "score": 575,
    "upvote_ratio": 0.96,
    "num_comments (reported by reddit)": 46,
    "url": "https://www.macrumors.com/2025/03/17/apples-m3-ultra-runs-deepseek-r1-efficiently/",
    "id": "1jdcx7t",
    "selftext": "",
    "comments": [
        {
            "author": "AshuraBaron",
            "body": "This was expected when they revealed the specs. Good to see it confirmed though. Impressive machine for large LLM's. Pricey to get there, but probably cheaper than renting out a big server.",
            "score": 235,
            "replies": [
                {
                    "author": "gildedbluetrout",
                    "body": "Yeah Dave 2d pointed out it really depends on what you\u2019re planning to do. 14 grand buys a lot of server time.",
                    "score": 77,
                    "replies": [
                        {
                            "author": "accidental-nz",
                            "body": "The Mac Studio isn\u2019t used up and disappears like server rental time. It\u2019s still worth something afterwards and can be on-sold. \n\nSo the better comparison would be the depreciation in a given period of time versus rental for the same period.",
                            "score": 58,
                            "replies": [
                                {
                                    "author": "animealt46",
                                    "body": "In fairness, Mac Studio also doesn't get new hardware upgrades like server rental time. So you win some and lose some.",
                                    "score": 21,
                                    "replies": [
                                        {
                                            "author": "psaux_grep",
                                            "body": "That\u2019s about the worst argument made. \n\nIt\u2019s like saying taxis are better than owning your own car because taxis get upgraded all the time. \n\nThe question is \u00abhow much do you have to drive before driving your own car is more economical than always taking a taxi?\u00bb",
                                            "score": -15,
                                            "replies": [
                                                {
                                                    "author": "notmyrlacc",
                                                    "body": "There\u2019s arguments both for and against. Buying hardware locks you in and also is an upfront expense. Server time is spread out and can also be scaled if your needs change as you go. \n\nCompanies will do both for their own use case.",
                                                    "score": 17,
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "NotRoryWilliams",
                                    "body": "The thing about rental server time is that it keeps producing profit, and if it goes well, the customer becomes permanently dependent at which point you can raise the rent for even more profit. It's a great business model.\n\nSell a computer once, and if it's powerful enough, you may not be able to extract additional revenue from that customer for years, and there's even a risk that they will buy their next one from someone else.\n\nThis is why we need complex AI models to do things that even just a decade ago everyone did on their local hardware without a second thought. Grammar check? You don't need a simple library of rules that can run on a 486, you need a massive 4 terabyte LLM that you could never hope to run on accessible hardware, because that's where the profit is long term. Never mind what the consumer actually wants or needs, we can convince them that they need 100x the computing power to accomplish essentially the same task 5% better or faster and with more interesting mistakes.",
                                    "score": 5,
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "SippieCup",
                            "body": "14 grand does not buy much more than a year of server time on a much weaker device. On AWS, Its $17k for 1 year of a 8 vCPU, 64GB Ram, and a v100 with 16GB Vram.\n\nAn A100 would be ~ 150k / year.",
                            "score": 1,
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "vanhalenbr",
                    "body": "You would need 6 H100s with NVLink, to get 802 TFlops of FP16 and the Mac Studio has 56.8 TFlops with maxed M3 Ultra. So we are talking about $180k (NVidia) againts $10k with the M3 Ultra",
                    "score": 15,
                    "replies": [
                        {
                            "author": "pirate-game-dev",
                            "body": "There are much cheaper options emerging including AMD AI Max with 128GB of RAM such as the Framework Desktop ($2000), nVidia Digit ($3000) with 128GB of RAM which is stackable x2.\n\nWe haven't seen what clustering the Frameworks will do yet but they have tons of Thunderbolt and LAN so it should scale very economically.",
                            "score": 23,
                            "replies": [
                                {
                                    "author": "notam00se",
                                    "body": "AMD software stack is a nightmare though. \n\nM4 Max in a laptop is faster rendering in Blender than desktop 7900 XTX \n\nhttps://opendata.blender.org/benchmarks/query/?device_name=AMD%20Radeon%20RX%207900%20XTX&device_name=Apple%20M4%20Max%20(GPU%20-%2040%20cores)&blender_version=4.3.0&group_by=device_name\n\nso thinking that AMD's low power solutions are going to compete with a mini or studio in full compute performance is a stretch",
                                    "score": -6,
                                    "replies": [
                                        {
                                            "author": "pirate-game-dev",
                                            "body": "We're not talking about rendering workloads or workloads suited to the Mini's 64GB of RAM. M4 Max certainly is comparable but that's  $3700 for 128GB of RAM so it's going to be expensive to cluster.\n\nA $10,000 Mac is unobtanium for most people and realistically it's probably more like $15,000+ everywhere else.  This is firmly in the \"company has to buy it for you to use\" bracket.\n\nA $2,000 PC - same hardware will be cheaper when other vendors release their models - is something people can actually buy.  And yeah it's going to perform worse of course but if you can't afford the *best* option it's a moot point.",
                                            "score": 11,
                                            "replies": [
                                                {
                                                    "author": "notam00se",
                                                    "body": "> expensive to cluster\n\nCluster 3 Framework desktop units at $6000 to match the performance of a single $3700 Mac Studio\n\nFramework 128GB desktop current batch is shipping in Q3, 128GB Mac Studio is available to pick up next week from store.",
                                                    "score": 0,
                                                    "replies": [
                                                        {
                                                            "author": "pirate-game-dev",
                                                            "body": "> Cluster 3 Framework desktop units at $6000 to match the performance of a single $3700 Mac Studio\n\nThat's 384GB of unified memory for $6000.  \n\nIf you need that memory, then you can't compare it to a single 128GB Mac Studio.  You need 3x them $11,100 or the $10,000 Mac Studio because that is the only one with >= 384GB of RAM.",
                                                            "score": 7,
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "TuxSH",
                            "body": "Not to mention the energy consumption (heat and money) difference which must be taken into account if running this for sustained period of time\n\n> Perhaps most impressively, the Mac Studio accomplishes this while consuming under 200 watts of power. Comparable performance on traditional PC hardware would require multiple GPUs drawing approximately ten times more electricity.\n\n2 kW is approx. what two electric heaters use when active, and 48 kWh (2x24h) is approx 10\u20ac to 15\u20ac in countries like France. If ran 24/7, not accounting for cooling, the energy consumption difference fully pay for the M3 Ultra in 3 years.",
                            "score": 5,
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "jonaskroedel",
            "body": "Yeah but 4 bit quantization is insane and will not give the full knowledge of the LLM... still impressive that a single Computer can run it locally...",
            "score": 105,
            "replies": [
                {
                    "author": "PeakBrave8235",
                    "body": "The full model is 8 Bit.\n\nIt isn\u2019t a large reduction. Notably, you can run the full 8 bit 671 B model with 2 M3U\u2019s using MLX and ExoLabs\n\nAlso, your characterization that it doesn\u2019t have the \u201cfull knowledge\u201d isn\u2019t exactly correct. It has all 671 B parameters, but they\u2019re reduced in size (8 bit vs 4 bit), so \u201caccuracy\u201d and quality is impacted.\u00a0",
                    "score": 89,
                    "replies": [
                        {
                            "author": "fleemfleemfleemfleem",
                            "body": "I don't like the term accuracy in this context, especially since \"precision\" is closer to what's happening.  \n\nIt's a reduction from possible 256 values per weight to 16. 2^8 vs 2^4. Quite a lot. \n\nIn terms of measurable stuff from models it tends to be things like increased perplexity, more hallucination, etc. Usually like a 10-20% drop in benchmarks. \n\nIt's still impressive given the number of video cards you'd need to run this on a typical PC setup, but need to be realistic about what it's able to do.",
                            "score": 8,
                            "replies": [
                                {
                                    "author": "joelypolly",
                                    "body": "Reduction is more like a few percentage points honestly.",
                                    "score": 5,
                                    "replies": []
                                },
                                {
                                    "author": "PeakBrave8235",
                                    "body": "My dude, people have already been running q4 models without issue.",
                                    "score": 2,
                                    "replies": [
                                        {
                                            "author": "fleemfleemfleemfleem",
                                            "body": "I didn't say they aren't. I'm saying that by definition the quantization is about the bit depth used to represent numbers, which is exponents of 2. So the change in precision is is large (even if the practical effect might be minor).",
                                            "score": 2,
                                            "replies": [
                                                {
                                                    "author": "PeakBrave8235",
                                                    "body": "I meant that they\u2019re running it and it\u2019s useful. Yes, it\u2019s less precise, but also it\u2019s not as large of a performance difference as the number would suggest\u00a0",
                                                    "score": 1,
                                                    "replies": [
                                                        {
                                                            "author": "fleemfleemfleemfleem",
                                                            "body": "I agree with that.",
                                                            "score": 2,
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "elamothe",
                            "body": "If that's not the nerdiest thing I'll read this year...",
                            "score": 12,
                            "replies": []
                        },
                        {
                            "author": "rustbelt",
                            "body": "So Apple can run the entire knowledge of the model with 8 bit giving it better precision over 4 bit?\n\nGoing to be unreal what happens by the m10",
                            "score": 1,
                            "replies": [
                                {
                                    "author": "PeakBrave8235",
                                    "body": "Apple can run the entire model in memory at 4 bit (never been done on a single desktop ever). You can fit the entire model in memory using ExoLabs to connect to Macs together.\u00a0",
                                    "score": 3,
                                    "replies": [
                                        {
                                            "author": "rustbelt",
                                            "body": "Wow. Thanks for the info. I wonder if it\u2019ll run magnus",
                                            "score": 1,
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "awesomeo1989",
                            "body": "Isn\u2019t Exolabs a scam with many fake claims and empty promises?",
                            "score": -3,
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "themixtergames",
            "body": "I wonder why they never mention prompt processing time... \ud83e\udd14",
            "score": 16,
            "replies": [
                {
                    "author": "fleemfleemfleemfleem",
                    "body": "I've had decent times with more reasonably sized models like Gemma3 12b on a 10core M4 (which is is shockingly good for a model of that size). \n\nI don't think that demo is meant to be practical -- very few people are going to buy $10k Mac Studios to run local LLMs. \n\nI see more as a proof of concept for where the technology can go in a few years. In the PC world, there's more stuff coming out with unified memory architectures too like the AMD strix halo chips. The 128gb framework desktop can be configured for about $2000. \n\nAlso been seeing some intel mini-PCs with 96gb of cheap stick ram running 70B models at \"useable\" speeds. \n\nPoints to a future of cheaper local LLM use overall with models of actually-useful size.",
                    "score": 6,
                    "replies": [
                        {
                            "author": "FightOnForUsc",
                            "body": "So you\u2019re running Gemma3 12b on M4? How is that? Any link to the instructions? I have an M4 Mac Mini and would be curious to try it",
                            "score": 2,
                            "replies": [
                                {
                                    "author": "fleemfleemfleemfleem",
                                    "body": "I downloaded LM Studio and it's one of the models offered for download. (Edit: it goes about 10 tokens per second, 3s to first token which is very usable.)\n\nI found it quite good. The answers are a little longwinded, so even though the context window is pretty long, it can run out in a relatively short conversation. \n\nI asked it for book recommendations and not of the series it came up with were hallucinations. \n\nI asked it a ridiculous question (please analyze the Bill and Ted movies through the lens of Foucault's ideas about personal and institutional power), and the answer was better than any comparably sized model I've tried that on, which usually start making up characters and things.",
                                    "score": 1,
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "MaverickJester25",
                            "body": "> I don't think that demo is meant to be practical -- very few people are going to buy $10k Mac Studios to run local LLMs. \n\nDisagree. I think the majority of buyers going for the higher-spec variants are doing so to run locally LLMs.",
                            "score": 1,
                            "replies": []
                        },
                        {
                            "author": "lesleh",
                            "body": "How much RAM on the M4?",
                            "score": 1,
                            "replies": [
                                {
                                    "author": "fleemfleemfleemfleem",
                                    "body": "16gb",
                                    "score": 2,
                                    "replies": [
                                        {
                                            "author": "lesleh",
                                            "body": "Oh nice! I'll have to give that a go then.",
                                            "score": 1,
                                            "replies": [
                                                {
                                                    "author": "fleemfleemfleemfleem",
                                                    "body": "I tried it with my m1pro as well. That one struggled with the 12b model but ran well with the 4b model which is actually quite good as well.",
                                                    "score": 2,
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "Mvnqaztaqoioqn473257",
            "body": "Dave2D\u2019s video:\n\nhttps://www.youtube.com/watch?v=J4qwuCXyAcU",
            "score": 24,
            "replies": []
        },
        {
            "author": "Ascendforever",
            "body": "I can see this maybe replacing some human, somewhere, in customer support. A lot cheaper than paying someone to simply provide information, and a lot more dynamic than an automated phone system or simple kiosk.",
            "score": 7,
            "replies": []
        },
        {
            "author": "cac2573",
            "body": "Shitty article of a video from a week ago that touches on it for like 15 seconds",
            "score": 5,
            "replies": []
        },
        {
            "author": "IndustryPlant666",
            "body": "What do people using these AIs actually use them for",
            "score": 4,
            "replies": [
                {
                    "author": "Sneedryu",
                    "body": "racist AI image generation that are censored by Big Tech",
                    "score": -10,
                    "replies": [
                        {
                            "author": "IndustryPlant666",
                            "body": "Doing some very important work",
                            "score": 7,
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "yaykaboom",
            "body": "Im dumb, what this mean?",
            "score": 0,
            "replies": []
        }
    ]
}