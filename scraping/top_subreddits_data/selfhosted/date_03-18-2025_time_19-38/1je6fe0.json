{
    "title": "How do you guard against supply chain attacks or malware in containers?",
    "author": "NTolerance",
    "subreddit": "selfhosted",
    "rank": 12,
    "score": 13,
    "upvote_ratio": 0.71,
    "num_comments (reported by reddit)": 27,
    "url": "https://www.reddit.com/r/selfhosted/comments/1je6fe0/how_do_you_guard_against_supply_chain_attacks_or/",
    "id": "1je6fe0",
    "selftext": "Back in the old days before containers, a lot of software was packaged in Linux distribution repos from a trusted maintainer with signing keys.  These days, a lot of the time it's a single random person with a Github account that's creating container images with some cool self hosted service you want, but the protection that we used to have in the past is just not there like it used to be IMHO.\n\nAll it takes is for that person's Github account to be compromised, or for that person to make a mistake with their dependencies and BAM, now you've got malware running on your home network after your next `docker pull`.  \n\nHow do you guard against this?  Let's be honest, manually reviewing every Dockerfile for every service you host isn't remotely feasible.  I've seen some expensive enterprise products that scan container images for issues, but I've yet to find something small-scale for self-hosters.  I envision something like a plug-in for Watchtower or other container updating tool that would scan the containers before deploying them.  Does something like this exist, or are there other ways you all are staying safe?  Thanks.",
    "comments": [
        {
            "author": "TW-Twisti",
            "body": "By always being terribly behind on updates, so other people run into stuff long before me.",
            "score": 22,
            "replies": [
                {
                    "author": "TW-Twisti",
                    "body": "Seriously though, my backups run on a different machine on a different update cycle that has almost no ingress and a write-only backup server (meaning an infected client could write encrypted/corrupted data, but not delete the older, good snapshots), and all my services run as rootless podman containers as their own users, so even a worst case container escape is still at least somewhat mitigated as without an additional privilege escalation exploit on the host, they can't touch other containers data.\n\nPlus, you know, fingers crossed, my daily driver with SSH access could get corrupted tomorrow, can't safeguard against everything in the world while retaining sensible usability.",
                    "score": 5,
                    "replies": [
                        {
                            "author": "Trysupe",
                            "body": "How to you do \"write-only\" backups? Which software/file system do you use?",
                            "score": 3,
                            "replies": [
                                {
                                    "author": "TW-Twisti",
                                    "body": "I use Restic and for this feature in particular the Restic REST server, which has an 'append only' mode which only lets you save new snapshots, not edit or delete existing ones. But I would expect any modern backup solution to support something of that sort.",
                                    "score": 1,
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "plaudite_cives",
            "body": "I always cross my fingers before running docker pull ;)\n\nI think docker hub scans the uploaded images for malware, that's enough for me",
            "score": 12,
            "replies": [
                {
                    "author": "behindmyscreen_again",
                    "body": "Linuxserver.io is a high reputation image creator too. They don\u2019t have everything, but they have a lot of stuff people like to use at home",
                    "score": 1,
                    "replies": []
                },
                {
                    "author": "Efficient_Ad_8020",
                    "body": "Pull and pray method, tried and true \ud83d\ude05",
                    "score": 0,
                    "replies": []
                }
            ]
        },
        {
            "author": "Simon-RedditAccount",
            "body": "All my containers don't have outgoing internet access, except very, very few ones. Cannot do much damage without connectivity.\n\nThis literally should be a default rule.\n\nPlus, all the usual stuff: file access controls, least possible privileges, locking, firewalling etc.",
            "score": 4,
            "replies": [
                {
                    "author": "ElevenNotes",
                    "body": "You would be surprised how little **internal: true** is mentioned in compose examples, especially from known publishers (who shall not be named).",
                    "score": 5,
                    "replies": []
                },
                {
                    "author": "Diligent_Ad_9060",
                    "body": "Do you allow recursive DNS? Most people miss that.",
                    "score": 1,
                    "replies": [
                        {
                            "author": "Simon-RedditAccount",
                            "body": "No, absolutely no connectivity.",
                            "score": 1,
                            "replies": [
                                {
                                    "author": "Diligent_Ad_9060",
                                    "body": "That helps. There are other side channels that can be used if your application is vulnerable to code execution vulnerabilities, but that would be quite a headache and maybe too time consuming to make it worth the effort.",
                                    "score": 2,
                                    "replies": [
                                        {
                                            "author": "Simon-RedditAccount",
                                            "body": "Realistically, all that ***common*** malware can do in an offline container is encrypting available non-volatile data with some embedded (thus, shared for many users) pubkey and demand ransom. Or providing malicious/compromised assets (i.e., a version of ~~Bit~~VaultWarden WebUI with some tweaked\u2122 JS code that sends a few extra XHR requests... :)\n\nWhile some advanced & sophisticated exfiltration techniques certainly do exist, I'm 100% sure that *a random Joe* (one who's not a C-level exec, journalist etc) won't encounter any of them *in action*.",
                                            "score": 1,
                                            "replies": [
                                                {
                                                    "author": "Diligent_Ad_9060",
                                                    "body": "Agreed. I'm just messing around in the details. If I had shell execution from the attacker perspective I'd probably use conditional sleep statements or a similar naive method for exfiltration. I wouldn't expect automated bots to try anything except the low hanging fruits.",
                                                    "score": 2,
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "BrenekH",
            "body": "The way to deal with potential malware in containers is the same way you deal with any potential intrusion point, isolate, give the least amount of privileges, and monitor for anomalies (plus all the other cyber security stuff). As you say auditing everything that comes in is a pipe dream and is infeasible for most people. So instead you have to lock down the machines and processes well enough that an intrusion can't spread. \n\nAutomated vulnerability scanners have too many false positives to really mean anything, and you'll never update or use anything if you take them as absolute truth. They exist right now as a way to point to potential problems, but it's up to you to investigate and identify if the report is valid or not. Which brings us back around to auditing, just slightly less of it.",
            "score": 5,
            "replies": []
        },
        {
            "author": "Flimsy_Complaint490",
            "body": "I don't because i run maybe 30 containers and they're all linuxserver or something that has 30k daily pulls. If somebody managed to hack into that, well, screw me i guess ?  I did however put in some effort to run everything rootless and minimal permissions, firewalled and so on, so that if anything gets pwned, blast radius is contained. \n\nBut If you really wanted to to be more protected, you need to become the packager yourself. Setup some local container registry (Harbor, docker-registry or zot are my togos), verify all your docker images and build everything locally. Make some git CI/CD pipeline to periodically pull updated dockerfiles or manually verify updated versions yourself, rebuild everything and do the \\`docker pull\\` command then. For more paranoia, you can sign your own images with cosign. Kubernetes can verify signatures, I have not figured out how to make compose do it. \n\nYou will still need to trust stuff at some point - are your base docker images not hacked ? Does the repo not have some weird mitm or backdoor by a rogue maintainer ? The less trust you have, the more you need to do yourself.",
            "score": 2,
            "replies": []
        },
        {
            "author": "DeadeyeDick25",
            "body": "Manually reviewing every dockerfile I host.",
            "score": 2,
            "replies": []
        },
        {
            "author": "KN4MKB",
            "body": "Most people here aren't concerned or educated enough about security to care. \nThey just pull any container from any guide that comes up first on Google, and if it runs they go on about their day. \n\nIt's a battle I've gave up on trying to explain to most people. \n\nI do pentesting, so maybe I take it a little too seriously most times.",
            "score": 5,
            "replies": [
                {
                    "author": "j-dev",
                    "body": "Most of the popular projects are on GitHub or otherwise have a huge amount of social proof that we take as a proxy for a well-deserved good reputation. I actively avoid projects with very few stars/downloads or a very short history. I also do this when installing VS Code extensions.\n\nOther than that, most end users and homelab enthusiasts aren\u2019t going to have the knowledge base for independently assessing whether a project is purposely malicious. The bigger risk here is good projects getting hijacked by malware they unintentionally installed from a third party package their project relies on.",
                    "score": 7,
                    "replies": [
                        {
                            "author": "Dangerous-Report8517",
                            "body": "Actually, in most cases that social proof is just a number of people have pulled it and it seemed to work so they moved on. Given how little code review some core parts of the broader Linux ecosystem get (*cough* xz *cough*) that isn't even close to solid verification. And, perhaps more importantly, you don't necessarily need to fully audit the code of every project you run, you just need to have an appropriate trust model where there's appropriate protections in place in case the version 0.2 of someone's random niche project turns out to have a security (or stability for that matter) issue.",
                            "score": 3,
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "HTTP_404_NotFound",
            "body": "firewall rules. logging. auditing.\n\nautomatic alerts for access failures.",
            "score": 2,
            "replies": []
        },
        {
            "author": "Pravobzen",
            "body": "Check out private image registries, such as Harbor.",
            "score": 1,
            "replies": []
        },
        {
            "author": "Dangerous-Report8517",
            "body": "\"All it takes is for that person's Github account to be compromised, or for that person to make a mistake with their dependencies and BAM, now you've got malware running on your home network after your next docker pull.\"\n\n\nThis isn't actually a new risk, and isn't really a supply chain risk at all. This is actually just security critical bugs in the application stack. Distro package managers provide a little bit of protection from this in that there's generally some degree of code review before repackaging but this is variable (Arch for instance expects the user to verify the package and offers a much larger number of less robustly verified packages as a result, Debian generally is very cautious, OpenBSD is ultra cautious and the base package repo is very strongly validated), and often mistakes can still slip through (see the recent malicious xz package incident which got into Fedora 41 during the beta phase among other distros).\n\n\nUltimately I think what you're really looking at here is the intrinsic risk of using niche software with a small user base and therefore less code review, and the answer there is segmentation - at the very least, make sure your Docker host is configured and hardened and that all containers are configured with least necessary permissions. Think extra, extra hard about containers that ask for high risk permissions like host networking or access to the Docker socket. The next step up is to subdivide your containers into separate VMs and have strong firewall restrictions between them, which is what I do. I'm happy enough in this instance to run some obscure, potentially iffy container but I'll run it on a VM that isn't also running Paperless or Nextcloud or something else with high value data.",
            "score": 1,
            "replies": []
        },
        {
            "author": "Arnwalden_fr",
            "body": "Is using docker with a non-root account not enough?  \nI think Podman has something like this with a different/etc/passwd and/etc/groups from the host.\n\nFor my part I test the containers on a VM before putting in prod and I make a backup (borg) before an update.",
            "score": 1,
            "replies": []
        },
        {
            "author": "GigabitISDN",
            "body": "This is why I don't use Docker unless I don't have a choice. I'd prefer to manage my own environment. It's true that any given update to any given application (Docker or otherwise) can be malicious, and the bottom line is that if you're going to run someone else's code, whether it's downloading source code or installing a package, you have to have some degree of trust in them. As you said you can inspect everything yourself, but that's beyond most people's abilities.\n\nYou can help mitigate this somewhat by only installing official containers. That performance-optimized Nextcloud container by \\~xXx\\_W33Dm4$t3r\\_xXx\\~ is probably worth passing on.",
            "score": -3,
            "replies": [
                {
                    "author": "Jazzy-Pianist",
                    "body": "LOL. Your statement can be applied to any complied exe, anything outside of docker(bare metal installs) and especially to big tech.\n\nDocker is amazing, and upgrades to other, safer orchestration/containerization can happen with time.\n\nYou are going to be someone's bitch. For me, I choose to be *dani-garcia(vaultwarden), invoice ninja, portainer, immich, nextcloud AIO, wazuh, npm, etc.*,'s bitch.\n\nWith a few security lines added to compose files for extra measure.",
                    "score": 1,
                    "replies": [
                        {
                            "author": "GigabitISDN",
                            "body": ">if you're going to run someone else's code, whether it's downloading source code or installing a package, you have to have some degree of trust in them",
                            "score": 1,
                            "replies": []
                        }
                    ]
                }
            ]
        }
    ]
}