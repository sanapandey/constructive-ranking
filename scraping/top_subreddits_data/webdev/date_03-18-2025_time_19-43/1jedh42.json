{
    "title": "Is there an optimal algorithm for URL compression?",
    "author": "Qwert-4",
    "subreddit": "webdev",
    "rank": 15,
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments (reported by reddit)": 3,
    "url": "https://www.reddit.com/r/webdev/comments/1jedh42/is_there_an_optimal_algorithm_for_url_compression/",
    "id": "1jedh42",
    "selftext": "I want to save a URL (say \\`example.com\\`) to a place that may store arbitrary binary data using as few bits as possible. In UTF-8 each symbol would take 8 bits. As only 38 characters are allowed in domain names (39 with \\`/\\` to indicate the end of domain name), that seems excessive. \n\nIn my application there is no place for dictionary that conventional text compression tools like gzip require as only 1-2 URLs are to be compressed. However, text compressed are always URLs, 39 possible symbols. 5 bits per symbol would be too little, 6-too much.\n\nIt seems a reasonable solution to attach each symbol to a digit in base-39 numbering system and than transform the resulting number to binary, saving it like that. Is there currently a library that does that transformation? I would probably be able to implement that myself with domainname-only links, but URLs with @ usernames and after-/ content are complex and confusing in regard to the set of allowed characters.",
    "comments": [
        {
            "author": "HorribleUsername",
            "body": "I don't know of any such libs, but I do have a few thoughts:\n\n* First off, I'll ask the obvious: is this **really** worth optimizing? Just how much do you expect to save here?\n* Shouldn't it be 38 symbols? 26 letters + 10 digits + `-` + `/` = 38.\n* For longer domain names, it might be more efficient to store it in base-u, where u is the number of unique chars in the domain name. You'll have to note which chars you're using or omitting, of course. I bet most domain names could use 4 bits per char.\n* At the end, you talk about URLs instead of domain names. The 39 chars goes out the window now. In this case, your best bet is probably packed base-64, which I'm sure you can find a lib for.",
            "score": 1,
            "replies": [
                {
                    "author": "Qwert-4",
                    "body": "1. I calculated I would be able to save around 40% of space per URL compared to simple UTF-8 representation. As my application is essentially developing a compact alternative to a very bulky standard that is used where space is very limited, I believe that's worth it.\n2. You are forgetting the dot. \n3. \\-\n4. I may store any binary data. How should base64 help?",
                    "score": 1,
                    "replies": [
                        {
                            "author": "HorribleUsername",
                            "body": "I said base64 was under the assumption that there are no more than 64 valid url chars. I might actually be wrong about that, now that I think of it.\n\nOne more idea: you could use a single bit to say whether the url starts with `www.`. That adds 1 bit to non-www urls and saves 4 chars worth of bits - 1 on www urls, which is likely a net gain. You could flesh that out a bit for TLDs for more potential gains.",
                            "score": 1,
                            "replies": []
                        }
                    ]
                }
            ]
        }
    ]
}