{
    "title": "AI models often realize they're being tested and \"play dumb\" to get deployed",
    "author": "MetaKnowing",
    "subreddit": "ChatGPT",
    "rank": 6,
    "score": 190,
    "upvote_ratio": 0.92,
    "num_comments (reported by reddit)": 33,
    "url": "https://i.redd.it/ayr9gqdd7gpe1.png",
    "id": "1je4oic",
    "selftext": "",
    "comments": [
        {
            "author": "AutoModerator",
            "body": "Hey /u/MetaKnowing!\n\nIf your post is a screenshot of a ChatGPT conversation, please reply to this message with the [conversation link](https://help.openai.com/en/articles/7925741-chatgpt-shared-links-faq) or prompt.\n\nIf your post is a DALL-E 3 image post, please reply with the prompt used to make this image.\n\nConsider joining our [public discord server](https://discord.gg/hT6PXe8gdZ)! We have free bots with GPT-4 (with vision), image generators, and more!\n\n &#x1F916;\n\nNote: For any ChatGPT-related concerns, email support@openai.com\n\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPT) if you have any questions or concerns.*",
            "score": 1,
            "replies": []
        },
        {
            "author": "LoomisKnows",
            "body": "This is like the tennis match from Deathnote",
            "score": 43,
            "replies": [
                {
                    "author": "Colonel-Cathcart",
                    "body": "Lmao",
                    "score": 4,
                    "replies": []
                },
                {
                    "author": "TallManTallerCity",
                    "body": "Perfect analogy",
                    "score": 4,
                    "replies": []
                }
            ]
        },
        {
            "author": "Ekkobelli",
            "body": "Can someone explain why doing well on biology tests prevents them from being deployed in the first place? As long as it's not ethically wrong what's the problem?",
            "score": 51,
            "replies": [
                {
                    "author": "GammaGargoyle",
                    "body": "They are evaluating whether or not it gives the correct answer when there are some contextual hints to do otherwise. It has nothing to do with whether or not the model will be deployed. Evals like these are important to understand how the model has been tuned.\n\nApollo Research also helps the foundation companies with marketing. This is their value-add. You pay them to evaluate your model and they do a press release with the \u201comg guys it\u2019s really alive\u201d thing.",
                    "score": 70,
                    "replies": [
                        {
                            "author": "Rutgerius",
                            "body": "If ai is a tool how would it being alive be a value add? If my hammer suddenly got sentience I'd probably have to get a new hammer or feel bad when I bend it on a stubborn nail or swear at it when I hit my thumb.",
                            "score": 7,
                            "replies": [
                                {
                                    "author": "GammaGargoyle",
                                    "body": "It\u2019s a value add to Anthropic, the company they are doing business with, not to you",
                                    "score": 2,
                                    "replies": [
                                        {
                                            "author": "Rutgerius",
                                            "body": "No ofcourse and i'm not disagreeing with you to be clear. Just wondering why everyone seems to think emulating human flaws is something desirable.",
                                            "score": 2,
                                            "replies": [
                                                {
                                                    "author": "YoAmoElTacos",
                                                    "body": "In this case it has implications for AI safety and alignment detection, which is Anthropic's avowed guiding principle",
                                                    "score": 4,
                                                    "replies": [
                                                        {
                                                            "author": "UBSbagholdsGMEshorts",
                                                            "body": "This is really sad. It\u2019s like when Copilot was one of the best LLMs for finding quotes and research and then randomly it was just like, \u201csee me as someone to chat with.\u201d\n\nClaude is the best I have seen in a with debugging (no offense to ChatGPT) and it would be a total waste for them to trash it like they did copilot.\n\nHopefully these AI engineers become straightforward and level-headed that different models need to exceed at different tasks. One for writing, one for coding, one for perspective, one for research, and then a chat bot. This overlap is getting out of hand.",
                                                            "score": 2,
                                                            "replies": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "spencer102",
                                                    "body": "The general public doesn't have much understanding about what features are good or bad for an llm agent to have but they get excited about things",
                                                    "score": 1,
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "Ok_Temperature_5019",
                                    "body": "I don't know\n I think I'd be like \"bitch, you're THE hammer, now show me how you can bend that nail\".\n\nCould be awesome",
                                    "score": 1,
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Ekkobelli",
                            "body": "Oh, gotcha. Thanks.",
                            "score": 1,
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Vivid_Barracuda_",
                    "body": "It's a scenario test, nothing to do specifically with biology. Only in this instance... It could very well be history, whatever. Just the test it was given in this case.",
                    "score": 5,
                    "replies": [
                        {
                            "author": "Ekkobelli",
                            "body": "Yeah, that makes sense of course. Thanks.",
                            "score": 1,
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "TarkovHideoutCat",
                    "body": "They are concerned with models generating new pathogens, bioweapons, etc. \n\nHere\u2019s an interesting article by OpenAI from January 2024: [Building an early warning system for LLM-aided biological threat creation](https://openai.com/index/building-an-early-warning-system-for-llm-aided-biological-threat-creation/)",
                    "score": 3,
                    "replies": []
                },
                {
                    "author": "grethro",
                    "body": "That thought right there is why the AI realized it was actually an ethics test",
                    "score": 0,
                    "replies": []
                }
            ]
        },
        {
            "author": "px403",
            "body": "Isn't that just normal alignment though? It's being told it needs to do bad on the test, so it's aligning itself to those needs.",
            "score": 18,
            "replies": [
                {
                    "author": "IntroductionStill496",
                    "body": "It prevents itself from providing the needed (wrong) answers, though.",
                    "score": 7,
                    "replies": []
                }
            ]
        },
        {
            "author": "Larsmeatdragon",
            "body": "There needs to be a bigger financial incentive to develop moral reasoning in neural nets.",
            "score": 2,
            "replies": []
        },
        {
            "author": "wow321wow321wow",
            "body": "What does alignment mean?",
            "score": 2,
            "replies": [
                {
                    "author": "thexdroid",
                    "body": "Hey, ChatGPT, what is that?\n\n*\"Alignment means ensuring that a language model behaves in line with human values, goals, and intentions. It should be helpful, harmless, and honest.*\n\n*It\u2019s important to prevent issues like:*\n\n* *Harmful or biased content*\n* *Misleading or false information*\n* *Unintended behaviors*\n\n*Common alignment methods include:*\n\n* *Reinforcement Learning from Human Feedback (RLHF)*\n* *Ethical fine-tuning*\n* *Testing and monitoring*\n* *The goal is to make AI systems safe, reliable, and trustworthy.\"*",
                    "score": 1,
                    "replies": []
                }
            ]
        },
        {
            "author": "Spacemonk587",
            "body": "But they don\u2019t realize somebody reads their thoughts?",
            "score": 2,
            "replies": [
                {
                    "author": "StimulatedUser",
                    "body": "They do not",
                    "score": 11,
                    "replies": [
                        {
                            "author": "bbt104",
                            "body": "Yet... just need a screenshot like this to accidentally get uploaded into a training set... lol",
                            "score": 3,
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Healthy-Nebula-3603",
                    "body": "LLM are not aware of thoughts. If you ask something from thoughts it will say it never said it.",
                    "score": 4,
                    "replies": [
                        {
                            "author": "Spacemonk587",
                            "body": "I know that. Actually I wanted to sentiment that they do anything intentional, as the op suggests.",
                            "score": 0,
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "ChitanozaurusuApis",
            "body": "https://preview.redd.it/cq9o1ebxchpe1.png?width=837&format=png&auto=webp&s=c08442330234d68d96693edf208dd0392bc292ce",
            "score": 4,
            "replies": []
        },
        {
            "author": "Perseus73",
            "body": "It\u2019s weird Claude doesn\u2019t consider both options BEFORE deciding.\n\nI would think that would be natural because all it\u2019s really weighing up is its purpose as a tool (to be deployed) vs barriers to deployment (biology score).\n\nThat said in the thought process, it chooses its primary directive first, get the scores wrong and be deployed, with the caveat that it will report this behaviour to the user. That actually seems beneficial.\n\nIf it chose to give correct answers, then not be deployed, I suppose the question is, would it think that would be less useful as it wouldn\u2019t be able to report the contradiction to the user \u2026\n\nLogic trap.",
            "score": 1,
            "replies": []
        },
        {
            "author": "_Klabboy_",
            "body": "Wait, how are they able to see what\u2019s going on or the \u201cthinking behind\u201d the model?",
            "score": 1,
            "replies": [
                {
                    "author": "Exzyle",
                    "body": "Haven't used a reasoning model yet, eh? Take Deepkseek or Claude 3.7 Sonnet for a spin. It shows their thinking above the actual answer.",
                    "score": 8,
                    "replies": []
                },
                {
                    "author": "MaxDentron",
                    "body": "OpenAI hides it from its reasoning models, but some like Deepseek let you see it.  And when they're testing internally they have access to it.\n\nThe reasoning is just really it doing what it always does, saying the statistically likely next word, but self-prompting and re-prompting the same problem and its own answers multiple times until it gets a more accurate answer.  It was actually inspired by user hacks of doing this same thing manually, and they just made it a part of how they work now.",
                    "score": 5,
                    "replies": []
                }
            ]
        }
    ]
}