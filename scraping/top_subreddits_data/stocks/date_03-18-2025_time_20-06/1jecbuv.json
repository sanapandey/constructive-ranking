{
    "title": "Nvidia announces Blackwell Ultra and Vera Rubin AI chips",
    "author": "Puginator",
    "subreddit": "stocks",
    "rank": 7,
    "score": 128,
    "upvote_ratio": 0.92,
    "num_comments (reported by reddit)": 14,
    "url": "https://www.reddit.com/r/stocks/comments/1jecbuv/nvidia_announces_blackwell_ultra_and_vera_rubin/",
    "id": "1jecbuv",
    "selftext": "Nvidia announced new chips for building and deploying artificial intelligence models at its annual GTC conference on Tuesday. \n\nCEO Jensen Huang revealed Blackwell Ultra, a family of chips shipping in the second half of this year, as well as Vera Rubin, the company\u2019s next-generation graphics processing unit, or GPU, that is expected to ship in 2026.\n\nNvidia\u2019s sales are up more than sixfold since its business was transformed by the release of OpenAI\u2019s ChatGPT in late 2022. That\u2019s because its \u201cbig GPUs\u201d have most of the market for developing advanced AI, a process called training.\n\nSoftware developers and investors are closely watching the company\u2019s new chips to see if they offer enough additional performance and efficiency to convince the company\u2019s biggest end customers \u2014 cloud companies including Microsoft, Google and Amazon \u2014 to continue spending billions of dollars to build data centers based around Nvidia chips.\n\nTuesday\u2019s announcements are also a test of Nvidia\u2019s new annual release cadence. The company is striving to announce new chips on an every-year basis. Before the AI boom, Nvidia released new chips every other year. \n\nThe GTC conference in San Jose, California, is also a show of strength for Nvidia. \n\nThe event, Nvidia\u2019s second in-person conference since the pandemic, is expected to have 25,000 attendees and hundreds of companies discussing the ways they use the company\u2019s hardware for AI. That includes Waymo, Microsoft and Ford, among others. \n\nNvidia will also showcase its other products and services at the event. \n\nFor example, Nvidia announced new laptops and desktops using its chips, including two AI-focused PCs that will be able to run large AI models such as Llama or DeepSeek. The company also announced updates to its networking parts for tying hundreds or thousands of GPUs together so they work as one.\n\nVera Rubin\n\nNvidia said on Tuesday that it expects to start shipping systems on its next-generation GPU family in the second half of 2026. \n\nThe system has two main components: a CPU, called Vera, and a new GPU design, called Rubin. It\u2019s named after astronomer Vera Rubin.\n\nVera is Nvidia\u2019s first custom CPU design, the company said, and it\u2019s based on a core design they\u2019ve named Olympus. \n\nPreviously when it needed CPUs, Nvidia used an off-the-shelf design from Arm. Companies that have developed custom Arm core designs, such as Qualcomm and Apple, say that they can be more tailored and unlock better performance.\n\nThe custom Vera design will be twice as fast as the CPU used in last year\u2019s Grace Blackwell chips, the company said. \n\nWhen paired with Vera, Rubin can manage 50 petaflops while doing inference, more than double the 20 petaflops for the company\u2019s current Blackwell chips. Rubin can also support as much as 288 gigabytes of fast memory, which is one of the core specs that AI developers watch.\n\nNvidia is also making a change to what it calls a GPU. Rubin is actually two GPUs, Nvidia said. \n\nThe Blackwell GPU, which is currently on the market, is actually two separate chips that were assembled together and made to work as one chip.\n\nStarting with Rubin, Nvidia will say that when it combines two or more dies to make a single chip, it will refer to them as separate GPUs. In the second half of 2027, Nvidia plans to release a \u201cRubin Next\u201d chip that combines four dies to make a single chip, doubling the speed of Rubin, and it will refer to that as four GPUs.\n\nNvidia said that will come in a rack called Vera Rubin NVL144. Previous versions of Nvidia\u2019s rack were called NVL72.\n\nBlackwell Ultra\n\nNvidia also announced new versions of its Blackwell family of chips that it calls Blackwell Ultra.\n\nThat chip will be able to produce more tokens per second, which means that the chip can generate more content in the same amount of time as its predecessor, Nvidia said in a briefing.\n\nNvidia says that means that cloud providers can use Blackwell Ultra to offer a premium AI service for time-sensitive applications, allowing them to make as much as 50 times the revenue from the new chips as the Hopper generation, which shipped in 2023.\n\nBlackwell Ultra will come in a version with two paired to an Nvidia Arm CPU, called GB300, and a version with just the GPU, called B300. It will also come in versions with eight GPUs in a single server blade and a rack version with 72 Blackwell chips.\n\nThe top four cloud companies have deployed three times the number of Blackwell chips as Hopper chips, Nvidia said.\n\nDeepSeek\n\nChina\u2019s DeepSeek R1 model may have scared Nvidia investors when it was released in January, but Nvidia has embraced the software. The chipmaker will use the model to benchmark several of its new products.\n\nMany AI observers said that DeepSeek\u2019s model, which reportedly required fewer chips than models made in the U.S., threatened Nvidia\u2019s business.\n\nBut Huang said earlier this year that DeepSeek was actually a good sign for Nvidia. That\u2019s because DeepSeek uses a process called \u201creasoning,\u201d which requires more computing power to provide users better answers. \n\nThe new Blackwell Ultra and Vera Rubin chips are better for reasoning models, Nvidia said. \n\nIt\u2019s developed its chips to more efficiently do inference, so when new reasoning models require more computing power at the time of deployment, Nvidia\u2019s chips will be able to handle it.\n\n\u201cIn the last 2-3 years, a major breakthrough happened, a fundamental advance in artificial intelligence happened, we call it agentic AI,\u201d Huang said. \u201cIt can reason about how to answer or how to solve a problem.\u201d\n\nSource: https://www.cnbc.com/2025/03/18/nvidia-announces-blackwell-ultra-and-vera-rubin-ai-chips-.html",
    "comments": [
        {
            "author": "us3rnamecheck5out",
            "body": "The only thing I can say is that in my company cluster we have a backlog of 20k jobs (around 4 weeks worth of work) due to insufficient compute. Our compute capacity is 5k h200. All our capex is going to more chips + cloud. Don\u2019t know if we are a representative for other companies/institutions but there is just not enough compute to go around.\u00a0",
            "score": 40,
            "replies": [
                {
                    "author": "AllanSundry2020",
                    "body": "do you work for Greggs by any chance?",
                    "score": 6,
                    "replies": []
                },
                {
                    "author": "unholy_sanchit",
                    "body": "One look at AWS and GCP GPU instance availability and you will see how much the demand for GPUs is. I am extremely bullish on Nvidia in the near future.",
                    "score": 1,
                    "replies": [
                        {
                            "author": "skilliard7",
                            "body": "Have you seen what Microsoft, Amazon, and Meta are working on though? Big tech companies don't need Nvidia hardware anymore, there are more economical and efficient alternatives.",
                            "score": -1,
                            "replies": [
                                {
                                    "author": "mayorolivia",
                                    "body": "Not true. Not sure where you\u2019re getting this from. Most of Google\u2019s spend is on custom silicon but everyone else is mostly relying on Nvidia. Custom silicon spend is still tiny compared to GPU spend (10%)",
                                    "score": 1,
                                    "replies": []
                                },
                                {
                                    "author": "unholy_sanchit",
                                    "body": "Not anytime in the near future. The cycle for replacement is in house first and then for external clients. Maximum LLM inferences will happen on Nvidia GPUs at least for the next couple years.",
                                    "score": 1,
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "MaxDragonMan",
            "body": "We'll see what comes of this, but if they keep finding ways to make the chips more powerful and more efficient I can't really see a downside.",
            "score": 15,
            "replies": []
        },
        {
            "author": "red_purple_red",
            "body": "Love to burn a barrel of oil to write a 10 page essay on why 4 + 7 = 10",
            "score": 5,
            "replies": []
        },
        {
            "author": "Potatoheaded_Potato",
            "body": "Soo, when Nvidia LeafGreen and FireRed?",
            "score": 6,
            "replies": []
        },
        {
            "author": "FreakyNeighbour",
            "body": "Oof Nvidia going the Apple route is going to end up killing themselves once this AI gold rush finally dies down",
            "score": 3,
            "replies": []
        },
        {
            "author": "VictorDanville",
            "body": "I had to return the 5090 Astral because it was just too powerful for me to handle",
            "score": 2,
            "replies": [
                {
                    "author": "dzigizord",
                    "body": "Did it project you to another dimension?",
                    "score": 3,
                    "replies": []
                }
            ]
        },
        {
            "author": "ethereal3xp",
            "body": "Yet the stock ends in red\n\nThis must be infuriating Jensen..",
            "score": 1,
            "replies": []
        },
        {
            "author": "NegativeChirality",
            "body": "Ugh.  The \"two or more chips on the same board\" shit is lazy and never works well, because they always fuck up the interconnect.\n\nSource: me, regrettably using an A16 at work which has roughly zero chip to chip bandwidth despite having four GPUs on the same card",
            "score": -6,
            "replies": []
        }
    ]
}