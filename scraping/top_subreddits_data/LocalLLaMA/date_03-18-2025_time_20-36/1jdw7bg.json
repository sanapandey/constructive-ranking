{
    "title": "After these last 2 weeks of exciting releases, the only thing I know for certain is that benchmarks are largely BS",
    "author": "ForsookComparison",
    "subreddit": "LocalLLaMA",
    "rank": 15,
    "score": 721,
    "upvote_ratio": 0.96,
    "num_comments (reported by reddit)": 118,
    "url": "https://i.redd.it/3lujka2ucdpe1.jpeg",
    "id": "1jdw7bg",
    "selftext": "",
    "comments": [
        {
            "author": "ttkciar",
            "body": "There are two problems with most benchmarks:\n\nFirst, models are trained to benchmax (of course).\n\nSecond, and this is less appreciated, benchmarks consist of tests which can be easily scored, which makes them very unlike the tasks we actually use LLM inference to do.\n\nI evaluate models with prompts which are more representative of typical tasks, which makes the results difficult to interpret. It's been two days since Gemma3-27B finished my tests, and I still haven't finished reviewing them (though that's in part because work has monopolized my time).",
            "score": 63,
            "replies": [
                {
                    "author": "LagOps91",
                    "body": "yeah pretty much this. even the creative writing benchmarks are largely \"did the LLM adhere to the prompt\" and not \"can the LLM actually write something that is worth reading\".\n\nThe real world usage is mostly to use the LLM as an assistant to soundboard ideas off of, not for the LLM to solve complex tasks on it's own yet. Sadly, that is hard to evaluate and therefore the models aren't optimized for important real-world use-cases.",
                    "score": 19,
                    "replies": [
                        {
                            "author": "Cergorach",
                            "body": "It also depends on what you use it for, sometimes it's more important to get what you asked for, sometimes it's better to get something 'nice'.\n\nWhat's also problematic is that different people expect different things. Some people are mad because it's not outputting literature, when neither is the average human or even the average writer...\n\nI'm for example very happy that full r1 outputs short 'evocative visuals' instead of a QwQ 32B that tries to write a novel right away...\n\nPersonally I use benchmarks as indicators, and then doing my own testing, for my own specific usecase.",
                            "score": 6,
                            "replies": []
                        },
                        {
                            "author": "aeroumbria",
                            "body": "There is a similar problem with image generation, where people laser focus on \"prompt adherence\" even though some models force some familiar styles or compositions (e.g. generating photo style whenever asked for challenging human poses) on the image whenever it encounters a hard prompt, rather than generating more natural images.",
                            "score": 1,
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Mescallan",
            "body": "If you are using local LLMs you should have your own benchmarks specific to your tasks",
            "score": 125,
            "replies": [
                {
                    "author": "LagOps91",
                    "body": "I think that's an unreasonable amount of work to do for the average use. maybe a hand full of prompts for a vibe check, but I am not making my own benchmarks. That is way too mucn work.",
                    "score": 56,
                    "replies": [
                        {
                            "author": "Everlier",
                            "body": "Vibe check is still a benchmark, just not a very stable or a scientific one.\n\nI cannot recommend Promptfoo enough for LLM testing: https://www.promptfoo.dev/docs/intro/. You can setup and run a specialised test in a single file, cery convenient.",
                            "score": 29,
                            "replies": [
                                {
                                    "author": "Spanky2k",
                                    "body": "Vibe check is exactly right and what is of most use in practice. This is *especially* true for anyone using LLMs for text generation type work.",
                                    "score": 1,
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Mescallan",
                            "body": "If you are just chatting a vibe check is enough, but if you are putting into software or using it as a value add, custom benchmarks and datasets are worth more than the time they take to make.",
                            "score": 15,
                            "replies": [
                                {
                                    "author": "DinoAmino",
                                    "body": "_Right_? So I guess by OPs measure we are above-average users :)",
                                    "score": 6,
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Western_Objective209",
                            "body": "I mean it's essentially testing. If you're building software without testing, it's probably going to suck if it reaches a significant level of complexity",
                            "score": 3,
                            "replies": []
                        },
                        {
                            "author": "keepthepace",
                            "body": "That would not be unreasonable for us, but the problem is that moving requirements and pipelines makes it impossible. I mean, we barely got rid of our slicing pipeline that dates back from when context windows were much smaller.",
                            "score": 2,
                            "replies": []
                        },
                        {
                            "author": "JFHermes",
                            "body": "Just take some normal tasks you use local models for (reasoning, summarising, re-writing etc) and perform sentiment analysis/fact checking/grading on the results with one of the cloud providers. It should take you like 3-4 hours to set this up.\n\nAlso some smaller local models are really good at specific tasks, as good as the closed sourced models but you need to run them through some dummy exercises. It's worth doing if local models *need* to be part of your workflow for whatever reason.",
                            "score": 2,
                            "replies": []
                        },
                        {
                            "author": "Enough-Meringue4745",
                            "body": "You really do need to quantify responses though",
                            "score": 1,
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "8Dataman8",
                    "body": "Yeah, I have a list of prompts I always run when a new hyped local LLM comes out.",
                    "score": 8,
                    "replies": [
                        {
                            "author": "palarsio",
                            "body": "Could you share your top 5 goto prompts that consistently break models?",
                            "score": 4,
                            "replies": [
                                {
                                    "author": "ForsookComparison",
                                    "body": "Don't. I personally rely upon 8Dataman8-Bench to judge new models and wouldn't want it to leak",
                                    "score": 7,
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "AD7GD",
                            "body": "...and then again a week later after the kinks are worked out and you find out the first quant you downloaded has a weird issue, and everyone has been using the wrong parameters...",
                            "score": 1,
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Armym",
                    "body": "How to benchmark for unstructured data?",
                    "score": 7,
                    "replies": [
                        {
                            "author": "popiazaza",
                            "body": "vibe benchmark \ud83d\udd25",
                            "score": 16,
                            "replies": [
                                {
                                    "author": "roselan",
                                    "body": "We will never get out of it. sigh.",
                                    "score": 6,
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Mescallan",
                            "body": "It depends on your output. \n\nI do unstructured data in, but I return JSON with fixed categories so it's a very clear pass/fail.\n\n\nMaybe reduce the logic to a classification problem? Or a binary \"does this data constant xyz\" across multiple domains.",
                            "score": 5,
                            "replies": []
                        },
                        {
                            "author": "Calcidiol",
                            "body": "You're doing some operations with it to achieve a result.  So take a representative scattering of use case example input data / tasks you care about, do those processes in a test scenario where you can repeat the same inputs and capture the output.  Verify that the output of some model that works for you on those test cases is whatever you'd consider good / correct.  \n\nThen save that setup as test cases and repeat it whenever you change the model or inference / processing configuration.  If you don't get the same (or comparably equivalently good) good results as the previous time(s) then call it a regression and investigate / fix / whatever.\n\nIf you can't easily automatically judge the correctness of the output from a varied model / configuration because it's subjective as to how it should be rated and it doesn't have to be identical each time (\"write a nice poem about a forest\") then, well, you've got a harder verification scenario.",
                            "score": 5,
                            "replies": []
                        },
                        {
                            "author": "thallazar",
                            "body": "LLM as judge?",
                            "score": 3,
                            "replies": []
                        },
                        {
                            "author": "latestagecapitalist",
                            "body": "You can only do it visually\n\nThis is why Sonnet has been so popular even though other models out-bench it\n\nBut anyone who uses it can feel the difference",
                            "score": 1,
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "LeastInsaneBronyaFan",
                    "body": "Currently working on that at the moment. But was too distracted with new hardware (with no numbers) so yeah.",
                    "score": 1,
                    "replies": []
                }
            ]
        },
        {
            "author": "maayon",
            "body": "It's funny and scary at the same time. Models are getting optimised for benchmarks instead of getting things done.\n\nI guess it's high time there are more personal benchmarks than models coming out. Infact benchmarks should step up the game realtime keep up with the model releases",
            "score": 35,
            "replies": [
                {
                    "author": "Super_Sierra",
                    "body": "I have an old RP card that is my go to, the formatting is two long sentence replies in a particular, first person style.\n\nNothing under 70b can really do it well, without telling the model what to do every fucking reply. \n\nThose low parameter models are really, really brain damaged, but people cope otherwise.",
                    "score": 19,
                    "replies": [
                        {
                            "author": "TheTerrasque",
                            "body": "That's my experience too. There's a very noticeable shift at 70b compared to the smaller models, and while the smaller models sometimes do well, they have a clear lack of  - for lack of a better word - understanding.",
                            "score": 6,
                            "replies": [
                                {
                                    "author": "xquarx",
                                    "body": "How do you feel the recent 20-30B models in 2025 compares to the 2024 summer/autumn releases of 70B? I don't have hardware to run the big ones, but to me seems the new ones have really improved what can be done in 20-30B range.",
                                    "score": 1,
                                    "replies": [
                                        {
                                            "author": "TheTerrasque",
                                            "body": "The gap is mostly the same. If you compare to llama2 70b - 2023 - then it's closer, but that 70b still has a lead even on today's 30b models when it's about subtlety and understanding. Or reacting more like how a human would. \n\nThe smaller models have gotten a lot better, or cleverer really, but they're still shallow. You see it from 8b to 30b models too, the 8b models will be shallower and less subtle than the 30b model.\n\nI think it's directly a result of the lesser number of parameters, making it incapable of reading deeper into things. You can see a small jump from 70b to 100+b models too, but less dramatic.",
                                            "score": 2,
                                            "replies": [
                                                {
                                                    "author": "xquarx",
                                                    "body": "What kind of hardware and qwant you run 70B on? As I've tested a bit with CPU offloading, but also not picking a brain dead qwant.",
                                                    "score": 1,
                                                    "replies": [
                                                        {
                                                            "author": "TheTerrasque",
                                                            "body": "q4 usually, cpu offloading or runpod.",
                                                            "score": 2,
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "maayon",
                            "body": "Mine is a JSON format and same. All small models are so bad except mistral and phi",
                            "score": 3,
                            "replies": [
                                {
                                    "author": "x0wl",
                                    "body": "Why aren't you using constrained generation for JSON?",
                                    "score": 1,
                                    "replies": [
                                        {
                                            "author": "maayon",
                                            "body": "The structure comes out correct with the given grammar. But the RHS values are so bad.",
                                            "score": 1,
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "No_Pilot_1974",
                            "body": "I doubt llama3.1 70b can do that but 8b can't. Even 8b is extremely good",
                            "score": -1,
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Regular_Working6492",
                    "body": "Aider\u2018s leaderboard is a good one",
                    "score": 1,
                    "replies": []
                },
                {
                    "author": "madaradess007",
                    "body": "it pretty much mirrors education  \nthis is a dead field that will yield nothing, i'm sorry we all wasted a lot of time",
                    "score": -5,
                    "replies": []
                }
            ]
        },
        {
            "author": "Ragecommie",
            "body": "Ah yes, the Q/A public data benchmarks you can train on...\n\nThat stopped making sense a while ago.",
            "score": 36,
            "replies": [
                {
                    "author": "No_Afternoon_4260",
                    "body": "Let say it's a benchmark to see what model retained this dataset the best. Isn't that worthless \ud83e\udd37",
                    "score": 5,
                    "replies": [
                        {
                            "author": "Ragecommie",
                            "body": "So we're basically testing this new form of lossy compression? If they market it like that I'm down, because these models are getting pretty good at it!",
                            "score": 10,
                            "replies": [
                                {
                                    "author": "No_Afternoon_4260",
                                    "body": "Hahaha yeah exactly! Reinventing jpeg for text on the all web lol",
                                    "score": 7,
                                    "replies": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "palarsio",
                    "body": "Living benchmarks are the way to go, creating a kind of SAT for LLms that changes every semester. It won't be perfect, but at least it will be harder for companies to cheat. Much like college, some models will genuinely learn, while others will just optimize for standardized test formats rather than real world",
                    "score": 3,
                    "replies": [
                        {
                            "author": "Ragecommie",
                            "body": "So, more Arc challenges?",
                            "score": 2,
                            "replies": [
                                {
                                    "author": "palarsio",
                                    "body": "Yeah, Arc is interesting, but according to light research (powered by Grok DeeperSearch) its dataset is public, so it will be overfitted soon. However arc prize will be renew this year, fresh distribution shift, new generalization tests. Every benchmark test should update every year as SAT changes as well\n\nhttps://preview.redd.it/2tlw60eorjpe1.png?width=950&format=png&auto=webp&s=6ec57d79cfd3372562a5e5212932880edbea2f03",
                                    "score": 1,
                                    "replies": [
                                        {
                                            "author": "Ragecommie",
                                            "body": "This is what I meant.\n\nMore novel benchmarks, not more training on available benchmark data...",
                                            "score": 1,
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Pyros-SD-Models",
                            "body": "So LiveBench?",
                            "score": 2,
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "AD7GD",
                    "body": "I would guess that most serious model creators are carefully scrubbing benchmarks from their training data, because they also want to use those benchmarks as validation. But that still influences the result, because if you're pretraining and you want to know if it's time to stop, you might run MMLU against a checkpoint and decide to keep going if you get a bad result. If you're doing GRPO to add reasoning to a model, and it's not getting better at ARC or MATH, you might go back and change your training setup until it is.",
                    "score": 3,
                    "replies": []
                }
            ]
        },
        {
            "author": "random-tomato",
            "body": "Based Meme fr.",
            "score": 41,
            "replies": []
        },
        {
            "author": "kovnev",
            "body": "We basically have a Volkswagen emissions scandal going on, but everyone's doing it.\n\nKnows the tests. Performs well on the tests. Then back to normal.",
            "score": 23,
            "replies": [
                {
                    "author": "Recoil42",
                    "body": ">We basically have a Volkswagen emissions scandal going on\n\nCalm down with this kind of rhetoric. No one's lying to a regulator or engaging in a cover-up, we don't need hyperbole in this thread. The two things aren't even remotely in the same league.",
                    "score": 3,
                    "replies": [
                        {
                            "author": "kovnev",
                            "body": "My point was they're lying to their customers. I don't care more about a regulator than us - the customers.\n\nDon't bother mentioning they're 'free'. Most of them will charge us as soon as they can.",
                            "score": 1,
                            "replies": [
                                {
                                    "author": "Recoil42",
                                    "body": ">My point was they're lying to their customers.\n\nYou aren't a customer, nor would it matter if you were. Testing to the benchmark is not the same as *cheating* a benchmark. You are describing two totally different concepts even in abstract.\n\nWhat Volkswagen did was falsify the data itself by using different programming on test cars than would be used in production. An *very rough* analogue here would be if someone performed a benchmark on a differently-tuned model than the one actually offered, and then claimed the benchmark model was the production model. By all means, if anyone behaves like that, nail them to the wall \u2014\u00a0*but that's not what people are complaining about.*\n\n>I don't care more about a regulator than us\n\nRegulations are, in this context, a proxy for 'us'. That's what regulators do. When Volkswagen was caught during Dieselgate, part of the remedy was appeasing consumers. Regulations don't exist for regulators' own benefits in this context, rather they're *advocates* for consumers. Your stated position is *fundamentally* self-contradiction.",
                                    "score": 1,
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "Expensive-Apricot-25",
            "body": "Benchmarks don\u2019t work. (As intended ideally)\n\nCompanies train their models to do well on the benchmarks, not generalize.\n\nThe goal is no longer the same. That\u2019s why new models only do well on one specific question",
            "score": 9,
            "replies": []
        },
        {
            "author": "Sad_Bandicoot_6925",
            "body": "And this is very hard to explain to the social media crowd. I think your should have your own benchmark and don't share it to public. \n\nOn every new model release we can create a thread here and call it the LocalLlama Benchmark. \n\nFrom our own benchmarks at NonBioS (specific to our 'agentic' usecase):\n\n1. Sonnet 3.5 \n2. Sonnet 3.7\n3. GPT-4o\n4. Nous Hermes Llama 405B\n5. Llama 405B\n6. Llama 70B, Gemini, Nemotron, DeepSeek, 405b 8 bit and every other flavour of the season we dont really care about.",
            "score": 6,
            "replies": []
        },
        {
            "author": "Only-Letterhead-3411",
            "body": "But that benchmarks are created by some professors in big universities, so they must be accurate. /s",
            "score": 22,
            "replies": [
                {
                    "author": "madaradess007",
                    "body": "all my normie friends are like this lol :D",
                    "score": 11,
                    "replies": []
                },
                {
                    "author": "Dyoakom",
                    "body": "Unironically, it's not an issue of the benchmarks necessarily but that people are gaming the system trying to score highly on them by both training on them and also only focusing on them and not real use cases. Goodhart's law in practice, when a measure becomes a target then it stops being a good measure. I don't think the benchmarks are problematic at all but rather our philosophy that they somehow are the ultimate metric of judging LLMs.",
                    "score": 3,
                    "replies": []
                },
                {
                    "author": "Pyros-SD-Models",
                    "body": "I mean, yes, I rather trust a Yann LeCun benchmark than Reddit\u2019s opinion every day of the week lol. It\u2019s not even a question.",
                    "score": 1,
                    "replies": []
                }
            ]
        },
        {
            "author": "yaosio",
            "body": "I've got the solution! Have thousands of very difficult benchmarks so training for the benchmarks also inadvertently results in a good general model.",
            "score": 4,
            "replies": [
                {
                    "author": "LagOps91",
                    "body": "yeah and then the companies would target the benchmarks, which they thing they could benchmax effectively to make headlines. like open ai did with arc-agi.",
                    "score": 1,
                    "replies": [
                        {
                            "author": "yaosio",
                            "body": "That's the point of it. Have so many hard benchmarks that the only way to get good at all of them is to make a good general benchmark.",
                            "score": 4,
                            "replies": [
                                {
                                    "author": "LagOps91",
                                    "body": "my point is that they will ignore most your hard benchmarks and hype up the model on those they were benchmaxing. you can't force them to do all the benchmarks and so they simply won't.",
                                    "score": 2,
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "quiteconfused1",
            "body": "Local llms are there because there is a need. Those that need, don't have an alternative.\n\n\ni use genma2 ( starting soon with 3) daily. Best llm that exists for my purposes.",
            "score": 3,
            "replies": []
        },
        {
            "author": "AmazinglyObliviouse",
            "body": "This but especially for VLMs. I just want a model that doesn't hallucinate 80% of the time when describing an image.",
            "score": 3,
            "replies": []
        },
        {
            "author": "a_beautiful_rhind",
            "body": "It never fails. Models that RP badly are usually awful at everything else.",
            "score": 3,
            "replies": []
        },
        {
            "author": "KedMcJenna",
            "body": "I have my own benchmarks for sentiment analysis, creative writing and editing. Gemma3:1B is consistently better by far than the formal benchmarks indicate it should be. I've been amazed by it. No, it won't be composing an opera or running a nuclear power station anytime soon, but just 6 months ago a 1b model (download: 815MB) could do little more than babble and word-associate. Now, this 1B is often at least on a par with a Llama 3B and at times approaching the performance of a 7B. \"*I know it sounds crazy but you gotta believe me!*\" territory I know.",
            "score": 3,
            "replies": []
        },
        {
            "author": "Aaaaaaaaaeeeee",
            "body": "I'd recommend they try to showcase models doing helpful tasks in multi-turn mode. Like looking at error messages when installing github projects, answering questions about docker, making sure it stayed on topic after 8K. You raised them on your farm, just share how healthy they are. Get someone. Are they very good for comprehension or code creation? How much instructions in bullet form can it follow exactly at 8K? \n\nPost GIFs, both people and benchmarks might miss how well your model can explain/cli tools, docker, whatever in one shot. so why not?",
            "score": 2,
            "replies": []
        },
        {
            "author": "latestagecapitalist",
            "body": "Benchmark have always and will always be gamed\n\nI used to work for a compiler company, the benchmarks were quite literally the primary target developers worked to -- thousands of optimisations just to squeeze more out against specific suites\n\nFor closed models the benchmarks people are using can't be hidden -- they literally go over the wire to the model vendor when important people get early access\n\nOpenAI has already seen all the benchmarks the commentators and AI leads at big companies are using -- many times -- and they've seen how they have be added to or tweaked over time\n\nThey likely watch every single prompt some important people make and tune just for them",
            "score": 2,
            "replies": []
        },
        {
            "author": "AppearanceHeavy6724",
            "body": "put a sleeping bum hugging a 3090 on a backseat; the dude just having fun with RP and silly funny fiction stories.",
            "score": 2,
            "replies": []
        },
        {
            "author": "jeffwadsworth",
            "body": "This is going on my gravestone as a fitting epitaph.",
            "score": 2,
            "replies": []
        },
        {
            "author": "perelmanych",
            "body": "Take benchmark results as indicator and test model on your specific usecases.\n\nUp to now QwQ output has never disappointed me. If problem turns out to be too complicated for it or I prefer to have second opinion there always free tires of thinking models like Grok and Gemini. Deepseek R1 in my usecase which is PhD math is even slightly inferior to QwQ. Non thinking Claude sometimes could surprise too.\n\nPS: For me it is not important whether a model gives you a correct answer on the first try. I am reading CoT to see if it comes up with some interesting approaches even if it fails to get them to the final result. I understand it is completely different story if you use a model in some application, so as I've said only your own tests can show whether it suits you.",
            "score": 3,
            "replies": [
                {
                    "author": "dorakus",
                    "body": "The benchmarks for the free tires of models are highly inflated",
                    "score": 2,
                    "replies": [
                        {
                            "author": "perelmanych",
                            "body": "I think that a lot of frustration comes from too aggressive quantization and wrong parameter settings, when users try to run these models locally. I am just now trying a new reasoning model by LG EXAONE Deep 32B and it produced flat crap until I saw a comment that said that it is very sensitive to a repetition penalty parameter. I had it 1.1 and the standard value is 1.0. Only after I changed it to default and set temperature to 0.6 it started to produce reasonable output.\n\nEdt: It still goes off the rails for hard prompts during reasoning. And I am sure there is still something wrong on my end.",
                            "score": 1,
                            "replies": [
                                {
                                    "author": "dorakus",
                                    "body": "Free TIRES. And they are INFLATED. I was making a joke.",
                                    "score": 1,
                                    "replies": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "AppearanceHeavy6724",
                    "body": "here is the task that trips QwQ (but not R1) and loops almost all non-reasoning models (for whatever strange reason, granite 3.1 8b had almost solved it, but  failed at the last steps):\n\nYou have a water reservoir with abundant water and three unmarked water jugs with known capacities of 5 liters, 6 liters, and 7 liters. The machine will only fill a completely empty jug when you place it inside. Special Note: You can empty a jug by pouring its contents into another jug, but if you pour water out without transferring it to another jug, as if pouring it on the ground\uff0cit will be considered \"waste\". How can you obtain exactly 8 liters of water using these 3 jugs while minimizing water waste?",
                    "score": 1,
                    "replies": [
                        {
                            "author": "perelmanych",
                            "body": "Just looking at the question I see that it is not just problem that needs to be solved, but it also should be proved that the obtained solution is optimal. If there is no solution with zero waste then difficulty of the problem escalates to a completely new level. This is hard not only for LLMs, humans struggle with proofs too. If DeepSeek R1 solves it, then Kudos to DeepSeek team, but I would not expect any even reasoning model to solve such type of questions. Most probably DeepSeek has somehow similar problem in the training set.",
                            "score": 1,
                            "replies": [
                                {
                                    "author": "AppearanceHeavy6724",
                                    "body": "What are you even talking about? It takes 3 minutes to come up with a solution optimal or not for a human; Majority of LLMs cannot come up _with any_ solution - they simply loop forever; even if they end up \"solving\" it comes massively inconsistent, full of elementary errors, ignoring some constraints of the task, like filling 5-liter jug with 7 liters of water. Current LLMs simply suck at tracking state of the objects.",
                                    "score": 1,
                                    "replies": [
                                        {
                                            "author": "perelmanych",
                                            "body": "Any solution is easy, but the task says optimal. In another reply I gave you output for some solution from QwQ. It didn't loop, just thought using 18.5k tokens. At a first glance it looks legit, but I didn't check it carefully.",
                                            "score": 1,
                                            "replies": [
                                                {
                                                    "author": "AppearanceHeavy6724",
                                                    "body": "> Deepseek R1 in my usecase which is PhD math is even slightly inferior to QwQ\n\nwell does not look like phd level to me R1 being inferior to QwQ. Reasoning models do not loop on this task, but still it is a very difficult task for them. Non-reasoning almost all loop.",
                                                    "score": 1,
                                                    "replies": [
                                                        {
                                                            "author": "perelmanych",
                                                            "body": "Different field. I mess with derivatives and integrals. I don't use any number theory, only real valued analysis, sometimes calculus of variations, probability theory and statistics. I am an economist.",
                                                            "score": 1,
                                                            "replies": [
                                                                {
                                                                    "author": "AppearanceHeavy6724",
                                                                    "body": "Anyway, I do not think  we should continue, as I agree with you anyway - QwQ is a good model.",
                                                                    "score": 1,
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "perelmanych",
                            "body": "Btw, here is s solution from QwQ. It used 18.5k tokens to solve it. I don't know whether it is optimal, but it looks as a legit candidate.\n\nTo obtain exactly 8 liters of water using the 5-liter, 6-liter, and 7-liter jugs while minimizing water waste, follow these steps:\n\n\n1. \\*\\*Fill the 7-liter jug (C)\\*\\*:\n\n   \\- \\\\( C = 7 \\\\), \\\\( A = 0 \\\\), \\\\( B = 0 \\\\)\n\n\n2. \\*\\*Pour from C into B until B is full\\*\\*:\n\n   \\- \\\\( B = 6 \\\\), \\\\( C = 1 \\\\), \\\\( A = 0 \\\\)\n\n\n3. \\*\\*Empty B into A\\*\\*:\n\n   \\- \\\\( A = 5 \\\\), \\\\( B = 1 \\\\), \\\\( C = 1 \\\\) (since pouring 5 liters into A leaves 1 liter in B)\n\n\n4. \\*\\*Pour the remaining 1 liter from B into C\\*\\*:\n\n   \\- \\\\( C = 2 \\\\), \\\\( B = 0 \\\\), \\\\( A = 5 \\\\)\n\n\n5. \\*\\*Empty A into C\\*\\*:\n\n   \\- \\\\( C = 7 \\\\), \\\\( A = 0 \\\\), \\\\( B = 0 \\\\) (since \\\\( 2 + 5 = 7 \\\\))\n\n\n\n6. \\*\\*Pour C into B until B is full\\*\\*:\n\n   \\- \\\\( B = 6 \\\\), \\\\( C = 1 \\\\), \\\\( A = 0 \\\\)\n\n\n\n7. \\*\\*Pour the 1 liter from C into A\\*\\*:\n\n   \\- \\\\( A = 1 \\\\), \\\\( C = 0 \\\\), \\\\( B = 6 \\\\)\n\n\n\n8. \\*\\*Fill C again\\*\\*:\n\n   \\- \\\\( C = 7 \\\\), \\\\( A = 1 \\\\), \\\\( B = 6 \\\\)\n\n\n\n9. \\*\\*Pour from C into B until B is full, then pour the remaining 4 liters from C into A\\*\\*:\n\n   \\- \\\\( B = 6 \\\\), \\\\( C = 3 \\\\), \\\\( A = 5 \\\\) (since \\\\( 7 - 4 = 3 \\\\))\n\n\n\n10. \\*\\*Now, A has 5 liters and C has 3 liters, totaling 8 liters\\*\\*:\n\n\\- \\\\( A = 5 \\\\), \\\\( C = 3 \\\\), \\\\( B = 6 \\\\)\n\n\n\nThe total water used is 14 liters, resulting in 6 liters of waste. However, the problem allows distributing the 8 liters between two jugs (A and C), which is acceptable.",
                            "score": 1,
                            "replies": [
                                {
                                    "author": "AppearanceHeavy6724",
                                    "body": "Yeah well steps 1 to 5 unnecessary; step 9 is unnessary and has incorrect description. One would think after 18.5k tokens it would arrive to elementary solution like pour 6 liter jug, pour 7 liter jug, pour from 7 into 5 liter jug and waste 5 liters.",
                                    "score": 1,
                                    "replies": [
                                        {
                                            "author": "perelmanych",
                                            "body": "If you want a proper solution to your question then now you should prove that there are no other solutions that will give you 8 litters and waste less than 5 litters. F-word such kind of problems. I tell you they are incredibly hard and this one comes straight from theory of numbers. Personally I would not even start it unless it explicitly says that there is a solution with zero waste.",
                                            "score": 1,
                                            "replies": [
                                                {
                                                    "author": "AppearanceHeavy6724",
                                                    "body": "You still miss the point - you've claimed R1 is weaker tha QwQ; but that was not true in my very simple case. You also keep saying that we need to prove that the most optimal solution is indeed requires only 5 liters; but this not what I've said - I merely pointed out that the better solution is obvious for a human and required 3 minutes for me to come op with this semi-optimal obvious solution.",
                                                    "score": 1,
                                                    "replies": [
                                                        {
                                                            "author": "perelmanych",
                                                            "body": "Yeah, may be my wording was not so good. I can't run DS R1 locally and when I used it online for free it was whether timeout or output very similar to QwQ. On the other hand output from Grok and Gemini was quite different and was good \"second opinion\" on a problem. That is why I stopped even try to use DS R1.",
                                                            "score": 1,
                                                            "replies": [
                                                                {
                                                                    "author": "AppearanceHeavy6724",
                                                                    "body": "fair enough, agree.",
                                                                    "score": 1,
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "custodiam99",
            "body": "No they are not, if we are talking about LiveBench. QwQ 32b is phenomenal.",
            "score": 2,
            "replies": [
                {
                    "author": "netsec_burn",
                    "body": "LiveBench needs to update their questions again. I've heard some mixed things about QwQ and 70% of the questions have been out since last November. Models could have trained on them extensively.",
                    "score": 1,
                    "replies": [
                        {
                            "author": "custodiam99",
                            "body": "LiveBench tested QwQ 32b two times. The second time it scored even higher.",
                            "score": 1,
                            "replies": [
                                {
                                    "author": "netsec_burn",
                                    "body": "With the same question bank, no? I'm saying we need an update for the questions. LiveBench was updating almost monthly, Jun 24, Jul 24, Aug 24, Nov 24. It's mid-Mar 25 (4 months later), there has been plenty of time for models to train on the public LiveBench question dataset and get inflated scores.",
                                    "score": 1,
                                    "replies": [
                                        {
                                            "author": "custodiam99",
                                            "body": "Well Phi-3 is not the number one on the list so I don't think it is serious issue. It is the most realistic leaderboard. I tried almost all local models and I can say that QwQ 32b is by far the best. It is unparalleled.",
                                            "score": 1,
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "darren457",
            "body": "Put the guy on the left in the middle and fill his old seat with \"people who shoehorn Local LLMs in their pipeline purely for marketing clout without actually solving any real world useful problem\".\n\nCongratulations, you now hate your life as much as your users hate you, lol.",
            "score": 2,
            "replies": []
        },
        {
            "author": "sdmat",
            "body": "https://preview.redd.it/xvlzmkw6ffpe1.png?width=1024&format=png&auto=webp&s=6b279c2d5b936ba200286352995362c3a96bd7e8",
            "score": 2,
            "replies": [
                {
                    "author": "ForsookComparison",
                    "body": "Hah this is dope",
                    "score": 3,
                    "replies": []
                }
            ]
        },
        {
            "author": "GreatBigSmall",
            "body": "What if I run my own custom instance on replicate can i join this club or do I just have to drop 8k on a setup before I can even start commenting?",
            "score": 1,
            "replies": [
                {
                    "author": "skarrrrrrr",
                    "body": "No Need 8K in reality because you can offload and batch with low end equipment",
                    "score": 1,
                    "replies": []
                }
            ]
        },
        {
            "author": "xor_2",
            "body": "Yeah, use LLMs in pipelines AND also try to finetune them not having right hardware for that. Great we have tools that allow that on consumer GPUs but their state is... volatile. Things break it seems.",
            "score": 1,
            "replies": []
        },
        {
            "author": "PeachScary413",
            "body": "Absolute shocker, I can't believe people would try to game a benchmark like that.",
            "score": 1,
            "replies": []
        },
        {
            "author": "05032-MendicantBias",
            "body": "I bet you one egg that all LLM model provider train on the benchmark.\n\nAs it's often said, when your metric become your objective, your metric becomes useless. Benchmarks should be secret and running on local instances to be meaningful.",
            "score": 1,
            "replies": []
        },
        {
            "author": "valdecircarvalho",
            "body": "THIS!",
            "score": 1,
            "replies": []
        },
        {
            "author": "ortegaalfredo",
            "body": "Benchmarks are not supposed to be a deterministic measure, but an approximation.\n\nThe problem is that when you have hundreds of billions of usd on the line and many jobs depending on a benchmark, the incentive is to cheat, and it's very easy to cheat on a benchmark. You don't even have to actually cheat but just cite benchmarks that put you on a positive light.\n\nI have my own benchmarks for the tasks I do and I know even those benchmarks are inaccurate.",
            "score": 1,
            "replies": []
        },
        {
            "author": "soteko",
            "body": "Yeah, true.\n\nAnd not just local, but and online llm's benchmarks are crap, lets say I've tried solving algebra math, and only one didn't make mistake.\n\nAll others are unreliable.\n\nBut in benchmarks is different.",
            "score": 1,
            "replies": []
        },
        {
            "author": "urekmazino_0",
            "body": "Just feel benchmark it",
            "score": 1,
            "replies": []
        },
        {
            "author": "Cool-Hornet4434",
            "body": "I always assume LLMs are trained on benchmarks, or at the very least they're fine tuned to perform well on them.  It's not necessarily bullshit, but it's not something I trust implicitly.  I spend a few days playing around with the model to find the limitations and sometimes there's limitations with the backend. \n\nFor example Running Gemma 3 27B Q5_K_S.GGUF on LM Studio is kinda slow at 8t/s.  I switch to Oobabooga and it's 18t/s now.  Same Context, and same model... just different backends. \n\nUnfortunately Oobabooga won't use multimodal models properly. I also noticed some issues with the way LM studio works, but it's too much to go into here.  \n\ntl;dr there's lots of variables that determine how well a model runs.  They're running their model with the best possible conditions to nail the benchmarks.  Your conditions may differ.",
            "score": 1,
            "replies": []
        },
        {
            "author": "Allseeing_Argos",
            "body": "My benchmark evaluates all my models on dick hardness when I use them for ERP.",
            "score": 1,
            "replies": []
        },
        {
            "author": "pigeon57434",
            "body": "i agree with the general sentiment of this but you have to admit QwQ-32B is like unbelievably good genuinely on par with R1 in 99% of scenarios despite running on only a 3090\n\nas for phone sized models beating R1 ya thats complete bullshit",
            "score": 1,
            "replies": []
        },
        {
            "author": "falconandeagle",
            "body": "For storywriting, I have a series of benchmarks I run everytime a new hype model comes along, I use my personal story writing software to check this as it allows me to use both local and openrouter.\n\nSo far all models <70b are pretty much useless if you are writing novels. All they are good for is writing short stories with heavy editing. All the benchmarks I see online for story writing are just useless when it comes to real world usage.\n\nOne of the most important things I look for in a model is spatial reasoning, and if the LLM is bad at that, any prose generated will have to be heavily edited. Also I have been super dissapointed with finetunes as a majority of them seem worse than the base model.\n\nFor as much progress has been made in coding and stem fields, creative writing has stagnated. We have gotten longer context but the prose generation has really not improved by much.\n\nFor coding its r1 or claude and nothing has come close so far. o1 pro might be good but I am not paying whatever absurd amount they are asking to use it.",
            "score": 1,
            "replies": [
                {
                    "author": "ForsookComparison",
                    "body": "How do you write a novel with an LLM?\n\nDo you load up a massive context and let it rip in one shot or do you try and generate page by page?",
                    "score": 1,
                    "replies": [
                        {
                            "author": "falconandeagle",
                            "body": "Step by step. I create a lorebook that has information on all my characters, locations, items. I include this lorebook info in all my prompts as AI often forgets this information, its usually around 2500 tokens as I try to keep it as consise as possible. Then I create a summary of all my chapters, so say for a 10 chapter novel thats around 2500 more tokens. Then I send the AI the previous 1500 words of the story form where it's currently at. If you follow these steps the prose generation works quite well, you will still need to edit quite a bit but its gets the gist right.\n\nSo overall in the middle of an average sized story the prompt is around 7k context and by the end it can go upto 12k context. Local LLM's start to really struggle with a 12k context input, however the SOTA models handle it fine, most of the time. You will still need to edit quite a bit, we are no where close to having the AI write a full novel by itself yet and if anyone tells you otherwise they are lying. Right now I am using Command A, Mistral Large, Grok, r1, gemini and Wizard 8x22b for my story writing purposes. Can't use claude and chatgpt as they are censored and my stories are mostly grim dark and post apocalyptic stuff and both those models are incapable of writing such stories because of their alignment.",
                            "score": 1,
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Sensitive-Tank-8189",
            "body": "I tried using my 7900 XTX in my pipeline, and it worked alright in terms of quality output, but I have literally millions of lines of text to analyze, and openAI lets you batch that out in 24hours, whereas I would be dead before my card could do it...",
            "score": 1,
            "replies": []
        },
        {
            "author": "Orolol",
            "body": "https://en.m.wikipedia.org/wiki/Goodhart%27s_law",
            "score": 1,
            "replies": []
        },
        {
            "author": "Virtualcosmos",
            "body": "I'm very happy with my gemma 3 27b and QwQ 32b locally, made my graphic cards extremely intelligent suddenly. Shame most of the time is processing videogames, Flux or Wan2.1",
            "score": 1,
            "replies": []
        },
        {
            "author": "Tuxedotux83",
            "body": "Meme was probably made by a guy who signed up for ChatGPT plus, type prompts and call them self \u201eAI evangelist\u201c (pun intended)\n\nTDLR; meme is either made by someone who have little clue or just intentionally made to be sarcastic \n\nSource: nobody think DS R1 can run on a smartphone",
            "score": -6,
            "replies": []
        }
    ]
}