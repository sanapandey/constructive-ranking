{
    "title": "Llama-3.3-Nemotron-Super-49B-v1 benchmarks",
    "author": "tengo_harambe",
    "subreddit": "LocalLLaMA",
    "rank": 6,
    "score": 74,
    "upvote_ratio": 0.94,
    "num_comments (reported by reddit)": 23,
    "url": "https://i.redd.it/9mswvzt3eipe1.png",
    "id": "1jef8pr",
    "selftext": "",
    "comments": [
        {
            "author": "vertigo235",
            "body": "I'm not even sure why they show benchmarks anymore. \n\nMight as well just say \n\nNew model beats all the top expensive models!!  Trust me bro!",
            "score": 40,
            "replies": [
                {
                    "author": "this-just_in",
                    "body": "While I generally agree, this isn't that chart.  Its comparing the new model against other Llama 3.x 70B variants, which this new model shares a lineage with.  Presumably this model was pruned from a Llama 3.x 70B variant using their block-wise distillation process, but I haven't read that far yet.",
                    "score": 33,
                    "replies": [
                        {
                            "author": "vertigo235",
                            "body": "Fair enough!",
                            "score": 1,
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "tengo_harambe",
                    "body": "It's a 49B model outperforming DeepSeek-Lllama-70B, but that model wasn't anything to write home about anyway as it barely outperformed the Qwen based 32B distill. \n\nThe better question is how it compares to QwQ-32B",
                    "score": 5,
                    "replies": [
                        {
                            "author": "soumen08",
                            "body": "See I was excited about QwQ-32B as well. But, it just goes on and on and on and never finishes! It is not a practical choice.",
                            "score": 1,
                            "replies": [
                                {
                                    "author": "Willdudes",
                                    "body": "Check your setting with temperature and such. \u00a0\nSetting for vllm and ollama here. \u00a0https://huggingface.co/unsloth/QwQ-32B-GGUF",
                                    "score": 2,
                                    "replies": [
                                        {
                                            "author": "soumen08",
                                            "body": "Already did that. Set the temperature to 0.6 and all that. Using ollama.",
                                            "score": 1,
                                            "replies": [
                                                {
                                                    "author": "Willdudes",
                                                    "body": "ollama run hf.co/unsloth/QwQ-32B-GGUF:Q4_K_M \u00a0 Works great for me",
                                                    "score": 2,
                                                    "replies": [
                                                        {
                                                            "author": "Willdudes",
                                                            "body": "No setting changes all built into this specific model",
                                                            "score": 2,
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "LagOps91",
            "body": "It's funny how on one hand this community complains about benchmaxing and at the same time completely discards a model because the benchmarks don't look good enough.",
            "score": 12,
            "replies": [
                {
                    "author": "foldl-li",
                    "body": "Yeah, the duality of this community, or human beings.",
                    "score": 1,
                    "replies": []
                }
            ]
        },
        {
            "author": "ResearchCrafty1804",
            "body": "According to these benchmarks, I don\u2019t expect it to attract many users. QwQ-32b is already outperforming it and we expect Llama-4 soon.",
            "score": 19,
            "replies": [
                {
                    "author": "Mart-McUH",
                    "body": "QwQ is very crazy and chaotic though. If this model keeps natural language coherence then I would still like it. Eg. I like L3 70B R1 Distill more than 32B QwQ,",
                    "score": 4,
                    "replies": []
                },
                {
                    "author": "ParaboloidalCrest",
                    "body": "I don't mind trying a llama3.3-like model with less pathetic quants (perhaps q3 vs q2 with llama3.3).",
                    "score": 5,
                    "replies": []
                }
            ]
        },
        {
            "author": "DinoAmino",
            "body": "C'mon man ... a link to something besides a pic?",
            "score": 7,
            "replies": []
        },
        {
            "author": "EugenePopcorn",
            "body": "A 70B equivalent that should fit on a single 32GB GPU? Cool.\u00a0",
            "score": 2,
            "replies": [
                {
                    "author": "Echo9Zulu-",
                    "body": "This guy gets it",
                    "score": 1,
                    "replies": []
                }
            ]
        },
        {
            "author": "Admirable-Star7088",
            "body": "I hope Nemotron-Super-49b is smarter than QwQ 32b, why else would anyone run a model that is quite a bit larger + less powerful?",
            "score": 2,
            "replies": []
        },
        {
            "author": "Calcidiol",
            "body": "That's IMO a bad graphic.  They compare it against reasoning and non reasoning models, great, but they don't show the present model's performance in BOTH reasoning and non reasoning modes distinctly.  So the only guess I can make is they perhaps used reasoning mode always (resulting in hopefully the best score for any problem case) in which case it's not so unexpected it'd 'win' against a non reasoning model but it might be much slower in doing so and it might not be indicative of this model's non reasoning performance.",
            "score": 1,
            "replies": []
        },
        {
            "author": "tengo_harambe",
            "body": "source: https://developer.nvidia.com/blog/build-enterprise-ai-agents-with-advanced-open-nvidia-llama-nemotron-reasoning-models/",
            "score": 1,
            "replies": []
        },
        {
            "author": "a_beautiful_rhind",
            "body": "So much wasted compute: https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1/tree/main/SFT/safety",
            "score": 1,
            "replies": []
        },
        {
            "author": "nother_level",
            "body": "So worse than QwQ with more parameters, pass",
            "score": 0,
            "replies": []
        },
        {
            "author": "Majestical-psyche",
            "body": "They waste compute for reseaching purposes... You don't learn unless if you do it.",
            "score": -2,
            "replies": []
        }
    ]
}