{
    "title": "Wen GGUFs?",
    "author": "Porespellar",
    "subreddit": "LocalLLaMA",
    "rank": 8,
    "score": 191,
    "upvote_ratio": 0.9,
    "num_comments (reported by reddit)": 48,
    "url": "https://i.redd.it/vv2vg9xbcgpe1.jpeg",
    "id": "1je58r5",
    "selftext": "",
    "comments": [
        {
            "author": "JustWhyRe",
            "body": "Seems actively in the work, at least text version. Bartowski\u2019s\u00a0at it.\n\n[https://github.com/ggml-org/llama.cpp/pull/12450](https://github.com/ggml-org/llama.cpp/pull/12450)",
            "score": 15,
            "replies": [
                {
                    "author": "BinaryBlitzer",
                    "body": "Bartowski, Bartowski, Bartowski! <doing my bit here>",
                    "score": 2,
                    "replies": []
                }
            ]
        },
        {
            "author": "noneabove1182",
            "body": "Text version is up  here :) \n\nhttps://huggingface.co/lmstudio-community/Mistral-Small-3.1-24B-Instruct-2503-GGUF\n\nimatrix in a couple hours probably",
            "score": 27,
            "replies": [
                {
                    "author": "ParaboloidalCrest",
                    "body": "imatrix quants are the ones that start with an \"i\"? If I'm going to use Q6K then I can go ahead and pick it from lm-studio quants and no need to wait for imatrix quants, correct?",
                    "score": 2,
                    "replies": [
                        {
                            "author": "noneabove1182",
                            "body": "no, imatrix is unrelated to I-quants, all quants can be made with imatrix, and most can be made without (when you get below i think IQ2_XS you are forced to use imatrix)\n\nThat said, Q8_0 has imatrix explicitly disabled, and Q6_K will have negligible difference so you can feel comfortable grabbing that one :)",
                            "score": 3,
                            "replies": [
                                {
                                    "author": "ParaboloidalCrest",
                                    "body": "Downloading. Many thanks!",
                                    "score": 2,
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "thyporter",
            "body": "Me - a 16 GB VRAM peasant - waiting for a ~12B release",
            "score": 35,
            "replies": [
                {
                    "author": "Zenobody",
                    "body": "I run Mistral Small Q4_K_S with 16GB VRAM lol",
                    "score": 23,
                    "replies": [
                        {
                            "author": "martinerous",
                            "body": "And with a smaller context, Q5 is also bearable.",
                            "score": 4,
                            "replies": []
                        },
                        {
                            "author": "Zestyclose-Ad-6147",
                            "body": "Yeah, Q4\\_K\\_S works perfect",
                            "score": 1,
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "anon_e_mouse1",
                    "body": "q3 arent as bad as you'd think. just saying",
                    "score": 15,
                    "replies": [
                        {
                            "author": "SukinoCreates",
                            "body": "Yup, especially IQ3\\_M, it's what I can use and it's competent.",
                            "score": 7,
                            "replies": []
                        },
                        {
                            "author": "DankGabrillo",
                            "body": "Sorry for jumping in with a noob question here. What does the quant mean? Is a higher number better or a lower number?",
                            "score": 1,
                            "replies": [
                                {
                                    "author": "raiffuvar",
                                    "body": "Number of bits. \nDefault is 16bit. So, we removing lower bit to save vram, lower bit is often does not affect response. \nBut further compressing == more artifacts. \nLow number = less vram in trade of quality, although quality for q8/q6/q5 is okay, usually it just drop a few percent of quality.",
                                    "score": 0,
                                    "replies": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "-Ellary-",
                    "body": "I'm running MS3 24b at Q4KS with Q8 16k context at 7-8tps.  \n\"Have some faith in low Qs Arthur!\".",
                    "score": 1,
                    "replies": []
                }
            ]
        },
        {
            "author": "AllegedlyElJeffe",
            "body": "Seriously! I even looked into trying to make one last night and realized how ridiculous that would be.",
            "score": 4,
            "replies": []
        },
        {
            "author": "Su1tz",
            "body": "Exl users...",
            "score": 5,
            "replies": []
        },
        {
            "author": "ed_ww",
            "body": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF",
            "score": 4,
            "replies": []
        },
        {
            "author": "ZBoblq",
            "body": "They are already there?",
            "score": 7,
            "replies": [
                {
                    "author": "Porespellar",
                    "body": "Waiting for either Bartowski\u2019s or one of the other \u201cgo to\u201d quantizers.",
                    "score": 5,
                    "replies": [
                        {
                            "author": "noneabove1182",
                            "body": "Yeah they released it under a new arch name \"Mistral3ForConditionalGeneration\" so trying to figure out if there are changes or if it can safely be renamed to \"MistralForCausalLM\"",
                            "score": 5,
                            "replies": []
                        },
                        {
                            "author": "Admirable-Star7088",
                            "body": "I'm a bit confused, don't we first have to wait for added support to llama.cpp first, if it ever happens?\n\nHave I misunderstood something?",
                            "score": 5,
                            "replies": [
                                {
                                    "author": "maikuthe1",
                                    "body": "For vision, yes. For next, no.",
                                    "score": 2,
                                    "replies": []
                                },
                                {
                                    "author": "Porespellar",
                                    "body": "I mean\u2026. someone correct me if I\u2019m wrong but maybe not if it\u2019s already close to the previous model\u2019s architecture. \ud83e\udd37\u200d\u2642\ufe0f",
                                    "score": -1,
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Su1tz",
                            "body": "Does it differ from quantizer to quantizer?",
                            "score": 1,
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "AllegedlyElJeffe",
            "body": "I miss the bloke",
            "score": 7,
            "replies": [
                {
                    "author": "ArsNeph",
                    "body": "He was truly exceptional, but he passed on the torch. Bartowski, LoneStriker, and Mrmradermacher picked up that torch. Just Bartowski alone has given us nothing to miss, his quanting speeds are speed-of-light lol. This model not being quanted yet has nothing to do with quanters and everything to do with Llama.cpp support. Bartowski already has text only versions up",
                    "score": 5,
                    "replies": []
                },
                {
                    "author": "ThenExtension9196",
                    "body": "What happened to him?",
                    "score": 4,
                    "replies": [
                        {
                            "author": "Amgadoz",
                            "body": "Got VC money. Hasn't been seen since",
                            "score": 6,
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "danielhanchen",
            "body": "A bit delayed, but uploaded 2, 3, 4, 5, 6, 8 and 16bit text only GGUFs to [https://huggingface.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF](https://huggingface.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF) Base model and pther dynamic quant uploads are at [https://huggingface.co/collections/unsloth/mistral-small-3-all-versions-679fe9a4722f40d61cfe627c](https://huggingface.co/collections/unsloth/mistral-small-3-all-versions-679fe9a4722f40d61cfe627c)\n\nAlso dynamic 4bit quants for finetuning through [Unsloth](https://github.com/unslothai/unsloth) (supports the vision part for finetuning and inference) and vLLM: [https://huggingface.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-unsloth-bnb-4bit](https://huggingface.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-unsloth-bnb-4bit)\n\nDynamic quant quantization errors - the vision part and MLP layer 2 should not be quantized\n\nhttps://preview.redd.it/50a6xaivhipe1.png?width=1000&format=png&auto=webp&s=80b11d79e7f6ebb33e8925162bac1c74f8900380",
            "score": 2,
            "replies": [
                {
                    "author": "DepthHour1669",
                    "body": "Do these support vision?\n\nOr they do support vision once llama.cpp gets updated, but currently don\u2019t? Or are the files text only, and we need to re-download for vision support?",
                    "score": 1,
                    "replies": []
                }
            ]
        },
        {
            "author": "Reader3123",
            "body": "Bartowski got you\n\nAnd mradermacher",
            "score": 2,
            "replies": []
        },
        {
            "author": "foldl-li",
            "body": "Relax, it is ready with chatllm.cpp:\n\n    python scripts\\richchat.py -m :mistral-small:24b-2503 -ngl all\n\nhttps://preview.redd.it/4x38xwn9hgpe1.png?width=640&format=png&auto=webp&s=aed0198287dcc9e8a72921d94e05ced0d68d24f2",
            "score": 5,
            "replies": [
                {
                    "author": "FesseJerguson",
                    "body": "does chatllm support the vision part?",
                    "score": 1,
                    "replies": [
                        {
                            "author": "foldl-li",
                            "body": "not yet.",
                            "score": 1,
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "PrinceOfLeon",
            "body": "Nothing stopping you from generating your own quants, just download the original model and follow the instructions in the llama.cpp GitHub. It doesn't take long, just the bandwidth and temporary storage.",
            "score": 3,
            "replies": [
                {
                    "author": "brown2green",
                    "body": "Llama.cpp doesn't support the newest Mistral Small yet. Its vision capabilities require changes beyond architecture name.",
                    "score": 8,
                    "replies": []
                },
                {
                    "author": "Porespellar",
                    "body": "Nobody wants my shitty quants, I\u2019m still running on a Commodore 64 over here.",
                    "score": 13,
                    "replies": []
                }
            ]
        },
        {
            "author": "NerveMoney4597",
            "body": "Can it even run on 4060 8gb?",
            "score": 1,
            "replies": []
        },
        {
            "author": "a_beautiful_rhind",
            "body": "Don't you need actual model support before you get GGUFs?",
            "score": 1,
            "replies": []
        },
        {
            "author": "DedsPhil",
            "body": "I saw there are some gguf out there on hf but the ones I tried just don load. Anxiously waiting for ollama support too.",
            "score": 1,
            "replies": []
        },
        {
            "author": "Actual-Lecture-1556",
            "body": "Where's this meme image from -- Sing Street?",
            "score": 1,
            "replies": []
        },
        {
            "author": "Z000001",
            "body": "Now the real question: wen AWQ xD",
            "score": 1,
            "replies": []
        },
        {
            "author": "sdnnvs",
            "body": "Ollama: \n\nollama run hf.co/lmstudio-community/Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q3_K_L",
            "score": 1,
            "replies": []
        },
        {
            "author": "None",
            "body": "[deleted]",
            "score": 0,
            "replies": [
                {
                    "author": "adumdumonreddit",
                    "body": "new arch and mistral didn\u2019t release a llamacpp pr like Google did so we need to wait until llamacpp supports the new architecture before quants can get made",
                    "score": 3,
                    "replies": []
                },
                {
                    "author": "Porespellar",
                    "body": "Right? Maybe he\u2019s translating it from French?\n\nhttps://i.redd.it/9l8rwgv4kgpe1.gif",
                    "score": 2,
                    "replies": []
                }
            ]
        },
        {
            "author": "xor_2",
            "body": "Why not make them yourself ?",
            "score": -2,
            "replies": [
                {
                    "author": "Porespellar",
                    "body": "Because I can\u2019t magically create the vision adapter for one. I don\u2019t think anyone else has gotten that working yet either from what I understand. Only text works for now I believe.",
                    "score": 7,
                    "replies": []
                }
            ]
        }
    ]
}