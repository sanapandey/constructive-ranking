{
    "title": "NVIDIA\u2019s Llama-nemotron models",
    "author": "gizcard",
    "subreddit": "LocalLLaMA",
    "rank": 14,
    "score": 40,
    "upvote_ratio": 1.0,
    "num_comments (reported by reddit)": 6,
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1jedum8/nvidias_llamanemotron_models/",
    "id": "1jedum8",
    "selftext": "Reasoning ON/OFF. Currently on HF with entire post training data under CC-BY-4. https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b ",
    "comments": [
        {
            "author": "a_beautiful_rhind",
            "body": "The last one was interesting. Hope this one isn't also \"choose your own adventure\" locked.",
            "score": 3,
            "replies": []
        },
        {
            "author": "mellowanon",
            "body": "the last 70B nemotron was really creative, and the fine-tunes kept that creativity. I hope this new reasoning model is equally creative.",
            "score": 3,
            "replies": []
        },
        {
            "author": "ResearchCrafty1804",
            "body": "Did they share any benchmarks?",
            "score": 2,
            "replies": [
                {
                    "author": "gizcard",
                    "body": "there are some in model cards",
                    "score": 3,
                    "replies": []
                }
            ]
        },
        {
            "author": "Glittering-Bag-4662",
            "body": "Sick!",
            "score": 2,
            "replies": []
        },
        {
            "author": "Calcidiol",
            "body": "I wonder how well the larger one will work quantized with Q8/Q4/BNB4 etc.\n\nIt will be interesting to see how the bigger one compares to QWQ-32B, qwen-2.5-32b/72b, the new exaone 32B, mistral-small-3.1, gemma3-27b, basically the other newer 24B/32B/72B reasoning and not models.\n\nThe reasoning toggle is nice, lots of use cases can have the information whether to do that or not case by case and one doesn't have to wholly swap out to different models to have a choice in this case (or several others where there's some possible inference configuration to disable reasoning one way or another).",
            "score": 2,
            "replies": []
        }
    ]
}