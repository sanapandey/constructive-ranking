{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import praw\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# API credentials \n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id='Y5DdpLJDcxJPSAT7Vvmo-A',\n",
    "    client_secret='KZ83gqe2B-C3uJwlcjEI4e1gR_gpPw',\n",
    "    user_agent='test by u/Inner-Astronomer3735'\n",
    ")\n",
    "\n",
    "# constants \n",
    "max_depth = 30\n",
    "base_folder = 'thread_tests'\n",
    "\n",
    "\n",
    "if not os.path.exists(base_folder):\n",
    "    os.makedirs(base_folder)\n",
    "\n",
    "def fetch_comments(comment_forest, depth=0):\n",
    "\n",
    "    if depth > max_depth:\n",
    "        return []\n",
    "    \n",
    "    comment_trees = []\n",
    "\n",
    "    # loop through root nodes\n",
    "    for comment in comment_forest:\n",
    "        \n",
    "        # When there's too many comments the API collaposes some of them into MoreComments objects\n",
    "        # This bit of code is to expand those and get the comments. \n",
    "        if isinstance(comment, praw.models.MoreComments):\n",
    "            time.sleep(0.3)\n",
    "            # this is an API call (time.sleep with rate limit in mind, which is 100 API calls per minute I think)\n",
    "            more = comment.comments()\n",
    "            comment_trees.extend(fetch_comments(more, depth))\n",
    "            continue\n",
    "\n",
    "        dictionary = {\n",
    "                    \"author\": str(comment.author),\n",
    "                    \"body\": comment.body,\n",
    "                    \"score\": comment.score,\n",
    "                    \"replies\": fetch_comments(comment.replies, depth+1)\n",
    "                }\n",
    "        \n",
    "        comment_trees.append(dictionary)\n",
    "\n",
    "    return comment_trees\n",
    "\n",
    "def fetch_post(post_id): \n",
    "\n",
    "    post = reddit.submission(id = post_id)\n",
    "    comments_thread = post.comments\n",
    "\n",
    "    comments_processed = fetch_comments(comments_thread)\n",
    "\n",
    "    # create file path\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H-%M\")\n",
    "\n",
    "    subreddit_folder = os.path.join(base_folder, post.subreddit.display_name)\n",
    "    timestamp_folder = os.path.join(subreddit_folder, f'timestamp -- {timestamp}')\n",
    "\n",
    "    if not os.path.exists(timestamp_folder):\n",
    "        os.makedirs(timestamp_folder)\n",
    "\n",
    "    post_file = os.path.join(timestamp_folder, f'id -- {post.id}.json, timestamp -- {timestamp}.json')\n",
    "\n",
    "\n",
    "    # gather post data\n",
    "    post_data = {\n",
    "                \"title\": post.title,\n",
    "                \"author\": str(post.author),\n",
    "                \"subreddit\": post.subreddit.display_name,\n",
    "                \"score\": post.score,\n",
    "                \"upvote_ratio\": post.upvote_ratio,\n",
    "                \"num_comments (reported by reddit)\": post.num_comments,\n",
    "                \"url\": post.url,\n",
    "                \"id\": post.id,\n",
    "                \"selftext\": post.selftext,\n",
    "                \"comments\": comments_processed\n",
    "            }\n",
    "    \n",
    "    with open(post_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(post_data, file, indent=4)\n",
    "        print(f'Saved {post.id}.json')\n",
    "\n",
    "# Apparently n_posts has to be less than 100. \n",
    "def fetch_subreddit(subreddit_name, n_posts): \n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    post_ids = [post.id for post in subreddit.top(limit=n_posts)]\n",
    "\n",
    "    for post_id in tqdm(post_ids): \n",
    "        fetch_post(post_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:02<00:05,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved g53lxf.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:08<00:04,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved hoolsm.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved gftejm.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "fetch_subreddit('python', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
