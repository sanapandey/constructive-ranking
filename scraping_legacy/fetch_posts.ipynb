{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import praw\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# API credentials \n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id='Y5DdpLJDcxJPSAT7Vvmo-A',\n",
    "    client_secret='KZ83gqe2B-C3uJwlcjEI4e1gR_gpPw',\n",
    "    user_agent='test by u/Inner-Astronomer3735'\n",
    ")\n",
    "\n",
    "# constants \n",
    "max_depth = 30\n",
    "max_comments = 30000\n",
    "base_folder = 'data'\n",
    "\n",
    "\n",
    "if not os.path.exists(base_folder):\n",
    "    os.makedirs(base_folder)\n",
    "\n",
    "def fetch_comments(comment_forest, depth=0):\n",
    "\n",
    "    if depth > max_depth:\n",
    "        return []\n",
    "    \n",
    "    comment_trees = []\n",
    "\n",
    "    # loop through root nodes\n",
    "    for comment in comment_forest:\n",
    "        \n",
    "        # When there's too many comments the API collaposes some of them into MoreComments objects\n",
    "        # This bit of code is to expand those and get the comments. \n",
    "\n",
    "        if isinstance(comment, praw.models.MoreComments):\n",
    "\n",
    "            time.sleep(0.05)\n",
    "            # this is an API call (time.sleep with rate limit in mind)\n",
    "            more = comment.comments()\n",
    "            comment_trees.extend(fetch_comments(more, depth))\n",
    "\n",
    "            continue\n",
    "\n",
    "        dictionary = {\n",
    "                    \"author\": str(comment.author),\n",
    "                    \"body\": comment.body,\n",
    "                    \"score\": comment.score,\n",
    "                    \"replies\": fetch_comments(comment.replies, depth+1)\n",
    "                }\n",
    "        \n",
    "        comment_trees.append(dictionary)\n",
    "\n",
    "    return comment_trees\n",
    "\n",
    "# The input is a post object (not a post_id like it used to be, but the actual post object)\n",
    "# Timestamp is used as part of the directory title, for organization\n",
    "def fetch_post(post, rank, timestamp): \n",
    "\n",
    "    comments_thread = post.comments\n",
    "\n",
    "    comments_processed = fetch_comments(comments_thread)\n",
    "\n",
    "    # create file path\n",
    "    subreddit_folder = os.path.join(base_folder, post.subreddit.display_name)\n",
    "    timestamp_folder = os.path.join(subreddit_folder, timestamp)\n",
    "\n",
    "    if not os.path.exists(timestamp_folder):\n",
    "        os.makedirs(timestamp_folder)\n",
    "\n",
    "    post_file = os.path.join(timestamp_folder,f'{post.id}.json')\n",
    "\n",
    "\n",
    "    # gather post data\n",
    "    post_data = {\n",
    "                \"title\": post.title,\n",
    "                \"author\": str(post.author),\n",
    "                \"subreddit\": post.subreddit.display_name,\n",
    "                \"rank\" : rank,\n",
    "                \"score\": post.score,\n",
    "                \"upvote_ratio\": post.upvote_ratio,\n",
    "                \"num_comments (reported by reddit)\": post.num_comments,\n",
    "                \"url\": post.url,\n",
    "                \"id\": post.id,\n",
    "                \"selftext\": post.selftext,\n",
    "                \"comments\": comments_processed\n",
    "            }\n",
    "    \n",
    "    with open(post_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(post_data, file, indent=4)\n",
    "        # print(f'Saved {post.id}.json')\n",
    "\n",
    "# Apparently n_posts has to be less than 100. \n",
    "def fetch_subreddit(subreddit_name, n_posts): \n",
    "\n",
    "    skipped = 0\n",
    "    downloaded = 0\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # timestamp for directory name. (\n",
    "    # We freeze this timestamp as soon as we run the function \n",
    "    # so that it doesn't change the folder name even if this takes time to run.\n",
    "    timestamp = datetime.now().strftime(\"date %m-%d-%Y time %H-%M\")\n",
    "\n",
    "    # Do we want top or hot? \n",
    "    # I'm assuming the enumeration here corresponds to the actual rank, testing quickly it seemed to be true. \n",
    "    for rank, post in enumerate(subreddit.hot(limit=n_posts)):\n",
    "\n",
    "        if post.num_comments < max_comments:\n",
    "            \n",
    "            # print(f'Fetching post {post.id} (rank = {rank}, num_comments = {post.num_comments}).')\n",
    "            fetch_post(post, rank, timestamp)\n",
    "            downloaded += 1\n",
    "            if downloaded % 10 == 1: \n",
    "                print(f'Downloaded {downloaded} posts from {subreddit}.')\n",
    "\n",
    "        else: \n",
    "            print(f'Skipped post with id {post.id} because it has {post.num_comments} comments. (max_comments set to {max_comments}).')\n",
    "            skipped += 1\n",
    "            print(f'Skipped count: {skipped}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1 posts from communism.\n",
      "Downloaded 11 posts from communism.\n",
      "Downloaded 21 posts from communism.\n",
      "Downloaded 31 posts from communism.\n",
      "Downloaded 41 posts from communism.\n",
      "Downloaded 51 posts from communism.\n",
      "Downloaded 1 posts from socialism.\n",
      "Downloaded 11 posts from socialism.\n",
      "Downloaded 21 posts from socialism.\n",
      "Downloaded 31 posts from socialism.\n",
      "Downloaded 41 posts from socialism.\n",
      "Downloaded 51 posts from socialism.\n",
      "Downloaded 1 posts from LateStageCapitalism.\n",
      "Downloaded 11 posts from LateStageCapitalism.\n",
      "Downloaded 21 posts from LateStageCapitalism.\n",
      "Downloaded 31 posts from LateStageCapitalism.\n",
      "Downloaded 41 posts from LateStageCapitalism.\n",
      "Downloaded 51 posts from LateStageCapitalism.\n",
      "Downloaded 1 posts from Conservative.\n",
      "Downloaded 11 posts from Conservative.\n",
      "Downloaded 21 posts from Conservative.\n",
      "Downloaded 31 posts from Conservative.\n",
      "Downloaded 41 posts from Conservative.\n",
      "Downloaded 51 posts from Conservative.\n",
      "Downloaded 1 posts from Libertarian.\n",
      "Downloaded 11 posts from Libertarian.\n",
      "Downloaded 21 posts from Libertarian.\n",
      "Downloaded 31 posts from Libertarian.\n",
      "Downloaded 41 posts from Libertarian.\n",
      "Downloaded 51 posts from Libertarian.\n",
      "Downloaded 1 posts from Anarcho_Capitalism.\n",
      "Downloaded 11 posts from Anarcho_Capitalism.\n",
      "Downloaded 21 posts from Anarcho_Capitalism.\n",
      "Downloaded 31 posts from Anarcho_Capitalism.\n",
      "Downloaded 41 posts from Anarcho_Capitalism.\n",
      "Downloaded 51 posts from Anarcho_Capitalism.\n",
      "Downloaded 1 posts from MadeMeSmile.\n",
      "Downloaded 11 posts from MadeMeSmile.\n",
      "Downloaded 21 posts from MadeMeSmile.\n",
      "Downloaded 31 posts from MadeMeSmile.\n",
      "Downloaded 41 posts from MadeMeSmile.\n",
      "Downloaded 51 posts from MadeMeSmile.\n",
      "Downloaded 1 posts from DogTraining.\n",
      "Downloaded 11 posts from DogTraining.\n",
      "Downloaded 21 posts from DogTraining.\n",
      "Downloaded 31 posts from DogTraining.\n",
      "Downloaded 41 posts from DogTraining.\n",
      "Downloaded 51 posts from DogTraining.\n",
      "Downloaded 1 posts from ADHD.\n",
      "Downloaded 11 posts from ADHD.\n",
      "Downloaded 21 posts from ADHD.\n",
      "Downloaded 31 posts from ADHD.\n",
      "Downloaded 41 posts from ADHD.\n",
      "Downloaded 51 posts from ADHD.\n",
      "Downloaded 1 posts from stopdrinking.\n",
      "Downloaded 11 posts from stopdrinking.\n",
      "Downloaded 21 posts from stopdrinking.\n",
      "Downloaded 31 posts from stopdrinking.\n",
      "Downloaded 41 posts from stopdrinking.\n",
      "Downloaded 51 posts from stopdrinking.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# seems to weigh like 200kb and take ~1min per 1000 comments. \n",
    "# (I think there's such a thing as \"authenticating an API user\" which might improve rate limits, but takes some weeks to get approval).\n",
    "n = 60\n",
    "# general\n",
    "# fetch_subreddit('politics',n) ran, got 41, then too many requests error.\n",
    "# fetch_subreddit('PoliticalDiscussion',n) got 61. \n",
    "# left\n",
    "fetch_subreddit('communism', n) \n",
    "fetch_subreddit('socialism', n)\n",
    "fetch_subreddit('LateStageCapitalism', n)\n",
    "\n",
    "# right\n",
    "fetch_subreddit('Conservative', n)\n",
    "fetch_subreddit('Libertarian', n)\n",
    "fetch_subreddit('Anarcho_Capitalism', n)\n",
    "\n",
    "# possibly uncontroversial?\n",
    "fetch_subreddit('MadeMeSmile', n)\n",
    "fetch_subreddit('DogTraining',n)\n",
    "fetch_subreddit('ADHD',n)\n",
    "fetch_subreddit('stopdrinking',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
